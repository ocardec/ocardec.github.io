[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "I am passionate about harnessing the power of data and machine learning to unlock endless possibilities. With a background in military operations and intelligence analysis, I have dedicated my career to transforming raw data into actionable insights that drive innovation and growth.\nThroughout my journey, I have:\n\nTransformed Data into Insight: Leveraged advanced data analytics and machine learning techniques to uncover hidden patterns and trends, leading to strategic decision-making and operational efficiency.\nDriven Innovation: Spearheaded projects that integrate cutting-edge machine learning algorithms to solve complex problems, enhance customer experiences, and optimize processes.\nCollaborated for Success: Worked with cross-functional teams to design and implement data-driven solutions, ensuring seamless integration and maximum impact.\nAchieved Tangible Results: Delivered measurable improvements in key performance indicators, demonstrating the real-world impact of data-driven strategies.\n\nAs a data professional at my current company, I continue to explore the frontiers of data science and machine learning. My mission is to push the boundaries of what’s possible, using data as a catalyst for change and growth.\nWhen I’m not diving into dataframes and algorithms, you can find me reading, hiking or spending quality time with my wife and lovely kiddos; always eager to explore new horizons and build new memories.\n\nLet’s connect and explore how we can harness the power of data together!\n\n\n\nOscar Cardec"
  },
  {
    "objectID": "books/index.html",
    "href": "books/index.html",
    "title": "Recommended Books",
    "section": "",
    "text": "“Our job is obvious: We need to get out of the way, shine a light, and empower a new generation to teach itself and to go further and faster than any generation ever has.”\n~ Seth Godin\n\n\n\n\n\nApplied Predictive Modeling by Kuhn, Max, & Johnson, Kjell\n\n\nDesigning Data-Intensive Applications: The Big Ideas Behind Reliable, Scalable, and Maintainable Systems by Martin Kleppmann\n\n\n\nPractical Simulations for Machine Learning: Using Synthetic Data for AI by by Paris Buttfield-Addison, Mars Buttfield-Addison, Tim Nugent, & Jon Manning\n\n\nAdvanced R, Second Edition (Chapman & Hall/CRC The R Series) by Hadley Wickham\n\n\n\nDeep Learning (Adaptive Computation and Machine Learning Series) by Ian Goodfellow, Yoshua Bengio, & Aaron Courville\n\n\nData Engineering with Python: Work with massive datasets to design data models and automate data pipelines using Python by Paul Crickard\n\n\n\nPractical Statistics for Data Science: 50 Essential Concepts by Peter Bruce, & Andrew Bruce\n\n\nAn Introduction to Statistical Learning: with Applications in R by Gareth James, Daniela Witten, Trevor Hastie, & Robert Tibshirani\n\n\n\nThe Hundred-Page Machine Learning Book by Andriy Burkov\n\n\nMachine Learning Engineering by Andriy Burkov\n\n\n\nData Science from Scratch: First Principles with Python by Joel Grus\n\n\nHands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems\n\n\n\nData Science Projects with Python: A case study approach to gaining valuable insights from real data with machine learning, 2nd Edition by Stephen Klosterman\n\n\nThe Art of R Programming: A Tour of Statistical Software Design by Norman Matloft\n\n\n\nData Science for Business: What You Need to Know about Data Mining and Data-Analytic Thinking by Foster Provost & Tom Fawcett\n\n\nApplied Predictive Analytics: Principles and Techniques for the Professional Data Analyst by Dean Abbott\n\n\n\nA General Introduction to Data Analytics by João Moreira, Andre Carvalho, & Tomás Horvath\n\n\nData Science and Big Data Analytics: Discovering, Analyzing, Visualizing, and Presenting Data by EMC Education Services\n\n\n\nDecision Management Systems: A Practical Guide to Using Business Rules and Predictive Analytics by James Taylor\n\n\nSystems Analysis and Design Shelly Cashman by Scott Tilley & Harry Rosenblatt\n\n\n\nStorytelling with Data: A Data Visualization Guide for Business Professionals by Cole Nussbaumer Knaflic\n\n\nLearn Python 3 the Hard Way: A Very Simple Introduction to the Terrifying Beautiful World of Computer and Code by Zed Shaw\n\n\n\nPython for Data Analysis: Data Wrangling with Pandas, Numpy, and IPython by William McKinney\n\n\nR For Data Science: Import, Tidy, Transform, Visualize, and Model Data by Garrett Grolemund & Hadley Wickham\n\n\n\nMastering Shiny: Build Interactive Apps, Reports, and Dashboards Powered by R\n\n\nThe Proximity Principle: The Proven Strategy That Will Lead to a Career You Love\n\n\n\nSpark: The Definitive Guide: Big Data Processing Made Simple\n\n\nBuild a Career in Data Science\n\n\n\nHands-On Data Analysis with Pandas: A handbook for data collection, wrangling, analysis, and visualization\n\n\nHandbook of Parametric and Nonparametric Statistical Procedures, Fifth Edition"
  },
  {
    "objectID": "resources/index.html",
    "href": "resources/index.html",
    "title": "Recommended Resources",
    "section": "",
    "text": "Web Sites\n\n\nThe Algorithms\nData Science Resource Hub - SAS\nData Science Central\nData, AI & Machine Learning\nStorytelling with Data\n\n\n\nCourses\n\n\nIntroduction to Data Science | edx IBM\nThe Data Scientist’s Toolbox\nAnalyze Data with Pandas\nProfessional Certificate in Data Science | edx Harvard\nApplied Data Science Program: Leveraging AI for Effective Decision-Making | MiT\nDeep Learning | Deeplizard\nDeep Learning Toolbox | MathWorks\n\n\n\nProjects & Books\n\n\nBig Book of R\nData Science on the GCP\n100+ Python projects\nR Books Collection\n40+ Modern ML Tutorials\n\n\n\nVisualization Package/Libraries\n\n\nmatplotlib\nggplot2\nseaborn\nGoogle Charts\nGrafana\nDatawrapper\nVisual Cinnamon\n\n\n\nBI Tools & Platforms\n\n\nKNIME\nSAS Enterprise Miner\nIBM Cognos Analytics\nSplunk MLTK\nShiny\nMode\nOSF\nQuarto\nRStudio\nAnaconda\nOrange\nSpyder\nJupyter\n\n\n\nCommunity\n\n\nData Science Hangout by Posit\nThe Data Canteen by Ted Hallum\nData Humans Podcast by Libby Heeren\nData Science Leaders Podcast by Kjell Carlsson\nData Skeptic Podcast by Kyle Polich\nSuper Data Science Podcast by Jon Krohn"
  },
  {
    "objectID": "posts/california_housing_prices/index.html",
    "href": "posts/california_housing_prices/index.html",
    "title": "House Market Analysis - Regression Model",
    "section": "",
    "text": "Introduction\nHere is a descriptive analysis and regression modeling while attempting to build a comprehensive housing market study. The main goal is to extract valuable insights related to any association between sales prices, home values, or other property features and how these contribute to the overall value of a property.\nData Ingestion\nThe ingested data pertains to properties found in a given California district and summary stats about them based on the 1990 census data. The data has been conditioned or curated and is available at Kaggle. A list of attributes are ordered below.\n\nThe dataset contains the following attributes:\n\nlongitude: Longitude of the house location.\nlatitude: Latitude of the house location.\nhousing_median_age: Median age of the houses.\ntotal_rooms: Total number of rooms in a block.\ntotal_bedrooms: Total number of bedrooms in a block.\npopulation: Population of a block.\nhouseholds: Number of households.\nmedian_income: Median income of households in the block.\nmedian_house_value: Median house value for households in the block (target variable).\nocean_proximity: Proximity of the block to the ocean.\n\n\n\nBroad overview of the data frame using skim(). Notice the number of missing values (207) across the total_bedrooms variable.\n\nskim(data, where(is.numeric))\n\n\nData summary\n\n\nName\ndata\n\n\nNumber of rows\n20640\n\n\nNumber of columns\n10\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n9\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\nlongitude\n0\n1.00\n-119.57\n2.00\n-124.35\n-121.80\n-118.49\n-118.01\n-114.31\n▂▆▃▇▁\n\n\nlatitude\n0\n1.00\n35.63\n2.14\n32.54\n33.93\n34.26\n37.71\n41.95\n▇▁▅▂▁\n\n\nhousing_median_age\n0\n1.00\n28.64\n12.59\n1.00\n18.00\n29.00\n37.00\n52.00\n▃▇▇▇▅\n\n\ntotal_rooms\n0\n1.00\n2635.76\n2181.62\n2.00\n1447.75\n2127.00\n3148.00\n39320.00\n▇▁▁▁▁\n\n\ntotal_bedrooms\n207\n0.99\n537.87\n421.39\n1.00\n296.00\n435.00\n647.00\n6445.00\n▇▁▁▁▁\n\n\npopulation\n0\n1.00\n1425.48\n1132.46\n3.00\n787.00\n1166.00\n1725.00\n35682.00\n▇▁▁▁▁\n\n\nhouseholds\n0\n1.00\n499.54\n382.33\n1.00\n280.00\n409.00\n605.00\n6082.00\n▇▁▁▁▁\n\n\nmedian_income\n0\n1.00\n3.87\n1.90\n0.50\n2.56\n3.53\n4.74\n15.00\n▇▇▁▁▁\n\n\nmedian_house_value\n0\n1.00\n206855.82\n115395.62\n14999.00\n119600.00\n179700.00\n264725.00\n500001.00\n▅▇▅▂▂\n\n\n\n\n\nExploratory Data Analysis\nMedian House Values Distribution\nFrequency distribution of the the main dependable variable of “median_house_value\" as given by the data steward website. Max number of bins limited to 50 for easy interpretation.\n\n# Visual\nggplot(data, aes(x = median_house_value)) +\n  geom_histogram(bins = 50, fill = \"darkblue\", col = \"grey\") +\n  labs(title = \"Distribution of Median House Values\", \n       x = \"Median House Value\", \n       y = \"Frequency\")+ \n  theme_minimal()\n\n\n\n\n\n\n\nAs illustrated, mean value distribution is right-skewed as the observations are mainly concentrated on the figures below the median. This could indicate that the sampled data has clusters of highly priced properties but not uniformly.\nCorrelation Matrix\nNotice the potential correlation across median_house_value and localization of the property as well as the representative income and total_rooms.\n\nnumeric_cols &lt;- data %&gt;% \n  select(-ocean_proximity)\n\ncorr_matrix &lt;- cor(numeric_cols)\n\ncorrplot(corr_matrix, \n         method = \"circle\", \n         type = \"upper\", \n         tl.cex = 0.9)\n\n\n\n\n\n\n\nHandling of Missing Values and Outliers\nAs previously mentioned, the total_bedrooms variable is missing some values, so I went ahead and to preserve the rest of the values, used the median of the entire attribute to replace those lacking the rooms data point. Additionally, I capped the data at +/- 3 standard deviations from the mean to have a more symmetrical data set. Lastly, given that I’m using “randomForest” there was no need to normalize (scale/center) the data.\n\nlibrary(DMwR2)\ndata$total_bedrooms[is.na(data$total_bedrooms)]&lt;- median(data$total_bedrooms, na.rm =TRUE)\n\ncap_outliers &lt;-function(x){\n  upper_limit &lt;- mean(x)+3*sd(x)\n  lower_limit &lt;- mean(x)-3*sd(x)\n  x &lt;- ifelse(x &gt; upper_limit, upper_limit, x)\n  x &lt;- ifelse(x &lt; lower_limit, lower_limit, x)\n  return(x)\n  }\n\nnumeric_columns &lt;- sapply(data,is.numeric)\n\ncapped_data &lt;- data\ncapped_data[numeric_columns]&lt;- lapply(capped_data[numeric_columns], cap_outliers)\n\nsummary(capped_data)\n\n   longitude         latitude     housing_median_age  total_rooms  \n Min.   :-124.3   Min.   :32.54   Min.   : 1.00      Min.   :   2  \n 1st Qu.:-121.8   1st Qu.:33.93   1st Qu.:18.00      1st Qu.:1448  \n Median :-118.5   Median :34.26   Median :29.00      Median :2127  \n Mean   :-119.6   Mean   :35.63   Mean   :28.64      Mean   :2560  \n 3rd Qu.:-118.0   3rd Qu.:37.71   3rd Qu.:37.00      3rd Qu.:3148  \n Max.   :-114.3   Max.   :41.95   Max.   :52.00      Max.   :9181  \n total_bedrooms     population     households     median_income   \n Min.   :   1.0   Min.   :   3   Min.   :   1.0   Min.   :0.4999  \n 1st Qu.: 297.0   1st Qu.: 787   1st Qu.: 280.0   1st Qu.:2.5634  \n Median : 435.0   Median :1166   Median : 409.0   Median :3.5348  \n Mean   : 523.4   Mean   :1391   Mean   : 487.7   Mean   :3.8362  \n 3rd Qu.: 643.2   3rd Qu.:1725   3rd Qu.: 605.0   3rd Qu.:4.7432  \n Max.   :1795.0   Max.   :4823   Max.   :1646.5   Max.   :9.5701  \n median_house_value ocean_proximity   \n Min.   : 14999     Length:20640      \n 1st Qu.:119600     Class :character  \n Median :179700     Mode  :character  \n Mean   :206856                       \n 3rd Qu.:264725                       \n Max.   :500001                       \n\n\nEncoding the Categorical Values\nMy last conditioning step prior to fitting the model was encoding the categorical variable of ocean_proximity to maximize data understanding, calculation, and value extraction by the model on this given attribute.\n\nlibrary(stats)\nclean_data &lt;- capped_data %&gt;%\n  dplyr::mutate(across(ocean_proximity, as.factor)) %&gt;%\n  model.matrix(~ ocean_proximity -1, data = .) %&gt;%\n  as.data.frame() %&gt;%\n  bind_cols(capped_data)\n\n# extra step to ensure clear naming convention\nclean_data &lt;- clean_data %&gt;% \n  dplyr::rename(\"ocean_proximity.1H.OCEAN\" = \"ocean_proximity&lt;1H OCEAN\",\n         \"ocean_proximity.INLAND\" = \"ocean_proximityINLAND\",\n         \"ocean_proximity.ISLAND\" = \"ocean_proximityISLAND\",\n         \"ocean_proximity.NEAR.BAY\" = \"ocean_proximityNEAR BAY\",\n         \"ocean_proximity.NEAR.OCEAN\" = \"ocean_proximityNEAR OCEAN\")\n\n# Remove the original categorical column\nclean_data &lt;- clean_data %&gt;% select(-ocean_proximity)\n\nModel Building and Diagnostics\nWith a cleaner data set, I was ready to start running my linear model and study the quality of the product. As recorded below, I used caret::createDataPartition package function to split the train and test data subsets using a 80:20 ratio.\nTrain - Test Split\n\n# data splitting\nset.seed(1212)\ntrainIndex &lt;- createDataPartition(clean_data$median_house_value, p = 0.8, list = FALSE)\ntrainData &lt;- clean_data[trainIndex, ]\ntestData &lt;- clean_data[-trainIndex, ]\n\nFit the Model\nFit Random Forest model to predict median house value.\n\nlibrary(randomForest)\n# Train a Random Forest model\nmodel &lt;- randomForest(median_house_value ~ ., data = trainData, na.action = na.omit,\n                      importance = TRUE, mtry = 4)\n\n# Model summary\nprint(model)\n\n\nCall:\n randomForest(formula = median_house_value ~ ., data = trainData,      importance = TRUE, mtry = 4, na.action = na.omit) \n               Type of random forest: regression\n                     Number of trees: 500\nNo. of variables tried at each split: 4\n\n          Mean of squared residuals: 2420723357\n                    % Var explained: 81.88\n\n\nObserve Model Characteristics\n\nvarImpPlot(model, main = \"Variable Importance Plot\", col = \"darkblue\")\n\n\n\n\n\n\n\nModel Assessment\n\n# model prediction\npredicted_values &lt;- predict(model, testData)\n\nActual vs. Predicted Viz\n\nlibrary(plotly)\npp &lt;- testData %&gt;% \n  ggplot(aes(median_house_value, predicted_values)) +\n  labs(title = \"California House Value Predicted vs. Actual\", \n       subtitle =\"Median Housing Price at 95% CI\")+\n  geom_point(alpha=0.4, col = \"darkblue\") + \n  geom_abline(intercept = 0, slope = 1, color = \"green\", linetype = \"dashed\")+\n  xlab('Actual Value') +\n  ylab('Predicted Value')+\n  theme_ipsum()\n\nggplotly(pp)\n\n\n\n\n\n\n# Calculate RMSE\nlibrary(ModelMetrics)\nmodel_rmse &lt;- rmse(predicted_values, testData$median_house_value)\nmodel_rmse\n\n[1] 47556.24\n\n\nResults\nThe Root Mean Square Error (RMSE) resulted in ~47,500 which can be further improved using feature extraction, rebuilding, and training the model. Keep in mind that this standard deviation of the residuals represent the distance between the regression line and the data points.\nSuggestions for Improving the Model\n\nFeature Engineering: Consider creating new features like room-to-population ratio or income categories.\nHyperparameter Tuning: Optimize the model’s parameters using grid search or cross-validation.\nDifferent Models: Experiment with other models like Gradient Boosting or XGBoost.\nEnsemble Methods: Combine predictions from multiple models to improve accuracy.\nHandling Outliers: Investigate and address potential outliers in the data.\n\nExtra Visualization\nThe following depiction shows population per latitude/longitude and the respective median price of a property.\n\n# Map depiction \nlibrary(maps)\n\nca_df &lt;- map_data(\"state\") %&gt;% filter(region ==\"california\")\ncounties &lt;- map_data(\"county\")\nca_county &lt;- subset(counties, region == \"california\")\n\nca_base &lt;- ggplot(data = ca_df, mapping = aes(x = long, y = lat, group = group)) + \n  coord_fixed(1.0) + \n  geom_polygon(color = \"black\", fill = \"gray\")\n\nca_map &lt;- ca_base +\n  geom_polygon(data = ca_county, fill = NA, color = \"gray\") +\n  geom_polygon(color = \"darkblue\", fill = NA)  \n\nbb &lt;- ca_map +\n  geom_jitter(data = clean_data, \n              aes(x = longitude, alpha = 0.4,\n              y = latitude, size = population, \n              col = median_house_value,\n              group = population))+\n  theme_minimal()+\n\n  labs(title = \"Population Distribution\", \n       subtitle =\"Median Housing Price\",\n       col = \"Median Price\",\n       size = \"Population\",\n       x = \"Longitude\",\n       y = \"Latitude\")\n\nggplotly(bb)"
  },
  {
    "objectID": "posts/multi_model_part1/index.html",
    "href": "posts/multi_model_part1/index.html",
    "title": "Aicraft’s Lateral Deviation - Part 1",
    "section": "",
    "text": "Introduction\nOn any given day, thousands of flights are maneuvering throughout the U.S. national airspace. All of these are constantly monitored by one entity, the Federal Aviation Administration (FAA). The FAA’s primary mission involves ensuring that flight operations are conducted efficiently, and to the highest levels of safety and security. In support of such endeavor, the continuous monitoring and accurate prediction of an aircraft position is a vital process across aeronautics and the FAA’s mission. Accurate forecasting of a flight could have a significant impact on businesses’ schedules, transportation logistics, or even protecting the environment. In today’s era of big data and technology advances monitoring of en-route flights its an imperative.\n\nDisclaimer: The views and opinions expressed in this report are those of the author and do not necessarily reflect the views or positions of any of the entities herein referred.\n\nThe following assessment builds out of a previously conducted analysis (Paglione et al. 2010) which documented a comprehensive evaluation of numerous aircraft’s lateral deviations. For context, lateral deviations enclose divergent measurements of an aircraft’s actual position in comparison to its authorized flight route. Here I assess and identify alternate options to sustain aerial operations management using modern machine learning algorithms to expose aircraft lateral anomaly detection. It employs innovative statistical analyses, compare results with the previous findings, and introduces a more sophisticated approach to improve the tracking of civil and military aviation on near real-time basis.\n\n\n\n\nData\nTo accomplish the aforementioned, historical data of numerous flights is utilized. This data involves different continuous and categorical observations including the aircraft’s altitude, measurement times, calculated distances from targeted route, lateral and vertical statuses and suggested corrective heading among other.\nThe original data encompasses 20 control centers within the Continental United States averaging around 500,000 observations per center. That aggregates to over 10,000,000 measurements nation-wide in less than a 24-hour window. Analysis of such figures result costly when it comes to computational power and time. For such reason, I take a sampled-data approach, assuming that the data is representative of the entire population and statistically speaking inferences may be applied to the entire population. The following diagram provides a basic depiction of involved variables.\n\n\n\n\nExploratory Data Analysis\nAs mentioned, the sampled data contains approximately 1.9 million flight observations from 4 specific Air Route Traffic Control Centers (ARTCC), namely, Chicago (ZAU), New York (ZNY), Miami (ZMA), and Los Angeles (ZLA). These observations contain attributes of an aircraft while cruising from one fix point or ARTCC to another, and the recorded data in increments of 10 seconds.\n\n\n\n\n\nNote: During exploratory steps the data is ingested and analyzed from a descriptive statistics standpoint. The 14 variables (different format types) are confirmed along with the total of 1.9 million observations. Also, notice how the “buildTime” variable is given as cumulative seconds, and the “latAdherStatus” as a character type. Notice, the “latAdherStatus” (lateral adherence status) variable which describes how distant the aircraft is from its authorized route (threshold recorded in nautical miles). This multivariate attribute is used as the target or dependable variable.\n\n\n\n\n\nstr(rawdf)\n\n'data.frame':   1988068 obs. of  14 variables:\n $ acid          : chr  \"AAL1016\" \"AAL1016\" \"AAL1016\" \"AAL1016\" ...\n $ cid           : chr  \"141\" \"141\" \"141\" \"141\" ...\n $ buildTime     : num  61560 61570 61580 61590 61600 ...\n $ TE            : chr  \"HMLP2\" \"HMLP2\" \"HMLP2\" \"HMLP2\" ...\n $ curAlt        : num  14766 15550 16233 16750 17000 ...\n $ stCenterTm    : num  61560 61560 61560 61560 61560 ...\n $ endCenterTm   : num  62180 62180 62180 62180 62180 ...\n $ mergeTime     : chr  \"null\" \"null\" \"null\" \"null\" ...\n $ vertCnfStatus : chr  \"inCnfDefault\" \"inCnfAscent\" \"inCnfAscent\" \"inCnfAscent\" ...\n $ vertCnfDist   : num  -26234 -25450 -24767 -24250 -24000 ...\n $ redLat        : num  -1.42 -1.72 -1.91 -1.96 -2.08 ...\n $ angle2NextFix : chr  \"10.71785671\" \"8.889666928\" \"8.00452716\" \"6.044758616\" ...\n $ latAdherStatus: chr  \"midNonConf\" \"outerNonConf\" \"outerNonConf\" \"outerNonConf\" ...\n $ artcc         : chr  \"ZAU\" \"ZAU\" \"ZAU\" \"ZAU\" ...\n\n\nNote: After gaining a basic understanding of the data set structure, I selected a particular set of variables, renamed them for easier understanding, factorized the labels from categorical values to a numerical for best calculation purposes, and defined a transformation function to convert time values from numeric to HMS format.\n\n# data selection\nmdata &lt;- rawdf %&gt;% \n  as_tibble() %&gt;% \n  dplyr::select(acid,cid,buildTime,stCenterTm,endCenterTm,redLat,angle2NextFix,\n                latAdherStatus, artcc)\n\n# factorization of lateral adherance status\nlevels &lt;-  c(\"innerInConf\", \"midInConf\", \"midNonConf\", \"outerNonConf\", \"endOfRoute\")\nlabels &lt;-  c(\"1\", \"2\", \"3\", \"4\", \"5\")\nmdata$latAdherStatus &lt;- factor(mdata$latAdherStatus, levels = levels, labels = labels)\n\n# variables renaming\nmaindf &lt;- dplyr::rename(.data = mdata,\n                        AircraftID = acid,\n                        ComputerID = cid,\n                        MeasureTaken = buildTime,\n                        ControlStartTm = stCenterTm,\n                        ControlEndTm = endCenterTm,\n                        LateralDeviation = redLat,\n                        CorrectionAngle = angle2NextFix,\n                        LateralStatus = latAdherStatus,\n                        ARTCC = artcc\n                        )\n\n# time conversion \npacman::p_load(lubridate, hms)\ntimeconvert &lt;- function(x){\n  # Transform time period from string to hms format ## \n  mt &lt;- seconds_to_period(x)\n  mtstring &lt;- sprintf('%02d:%02d:%02d', mt@hour, minute(mt), second(mt))\n  hms::as_hms(mtstring)}\n\n# trans with dplyr::mutate\ndf &lt;- maindf %&gt;% \n   dplyr::mutate(ARTCC = as.factor(ARTCC), \n    UniqueID = paste(AircraftID, ComputerID, sep = \"_\"), \n    Airline = substring(AircraftID, 1, 3),\n    Airline = as.factor(Airline),\n    MeasureTaken = as.numeric(MeasureTaken),\n    CorrectionAngle = as.character(CorrectionAngle),\n    LateralDeviation = as.double(LateralDeviation),\n    xMeasureTaken = timeconvert(MeasureTaken),\n    xControlStartTm = timeconvert(ControlStartTm),\n    xControlEndTm = timeconvert(ControlEndTm))\n\ndf$CorrectionAngle &lt;- as.double(df$CorrectionAngle)\n\n# exclusion of NAs introduced by coercion \ndf$CorrectionAngle[is.na(df$CorrectionAngle)] &lt;- max(df$CorrectionAngle, na.rm = TRUE) + 1\nhead(df, 10)\n\n# A tibble: 10 × 14\n   AircraftID ComputerID MeasureTaken ControlStartTm ControlEndTm\n   &lt;chr&gt;      &lt;chr&gt;             &lt;dbl&gt;          &lt;dbl&gt;        &lt;dbl&gt;\n 1 AAL1016    141               61560          61560        62180\n 2 AAL1016    141               61570          61560        62180\n 3 AAL1016    141               61580          61560        62180\n 4 AAL1016    141               61590          61560        62180\n 5 AAL1016    141               61600          61560        62180\n 6 AAL1016    141               61610          61560        62180\n 7 AAL1016    141               61620          61560        62180\n 8 AAL1016    141               61630          61560        62180\n 9 AAL1016    141               61640          61560        62180\n10 AAL1016    141               61650          61560        62180\n# ℹ 9 more variables: LateralDeviation &lt;dbl&gt;, CorrectionAngle &lt;dbl&gt;,\n#   LateralStatus &lt;fct&gt;, ARTCC &lt;fct&gt;, UniqueID &lt;chr&gt;, Airline &lt;fct&gt;,\n#   xMeasureTaken &lt;time&gt;, xControlStartTm &lt;time&gt;, xControlEndTm &lt;time&gt;\n\n\nNote: Next, I portray the distributions of some variables. Considering the following graphics, I anticipate significant variance across the target variable. The original data appears to be consistent along the mean, of course with the exception of some outliers.\n\n# Lateral Adherence Status Labels\npar(mfrow = c(1, 1))\nplot(df$LateralDeviation, type = \"l\", ylab = \"Nautical Miles\",\n     col = \"darkblue\",  main = \"Lateral Deviations\")\n\n\n\n\n\n\n\n\n# lateral deviation boxplot\npar(mfrow=c(2,2), mar=c(3,3,3,3))\nboxplot(df$LateralDeviation, outline = TRUE, border = \"darkred\", \n        ylab = \"Nautical Miles\", main = \"Boxplot Lateral Deviations\")\nboxplot(df$LateralDeviation, outline = FALSE, col = \"lightgray\", border = \"darkred\",\n        ylab = NULL, main = \"Boxplot Lateral Deviations\\n(No Outliers)\")\n# histogram target variable\nhist((df$LateralDeviation), main=\"Histogram Lateral Deviations\",\n     xlab=\"Nautical Miles\", border=\"white\", col = \"darkblue\", labels = FALSE)\n# Lateral Deviations Log2 Calc Histogram\nhist((log2(df$LateralDeviation)), main=\"Histogram Lateral Deviations\\n(Log2)\",\n     xlab=\"Nautical Miles\", ylab = NULL, border=\"white\", col = \"darkblue\", labels = FALSE)\n\n\n\n\n\n\n\nNote: The correction angle attribute refers to the degrees required to regain the proper heading in order to reach the next fix point associated with the flight plan. The following is a granular view of the correction angle data observations.\n\npar(mfrow=c(2,1))\n# Before\nhist((sel1$CorrectionAngle), main=\"Correction Angle Histogram\",\n     xlab=\"Degrees\", border=\"white\", col = \"darkred\", labels = FALSE)\n# After Log\nhist(log2(sel1$CorrectionAngle), main=\"Correction Angle Histogram\\n(Log2)\",\n     xlab=\"Degrees\", border=\"white\", col = \"darkblue\", labels = FALSE)\n\n\n\n\n\n\n\nNote: Next I display each ARTCC and their respective aircraft behaviors in relation to their assigned route. Bottom line, the data distribution maintains uniformity when I separate the levels of deviations by their centers.\n\n#Lateral Deviation/Status per ARTCC\npacman::p_load(gridExtra)\npar(mfrow = c(1,1))\n\nls1 &lt;- sel1 %&gt;% \n  filter(LateralStatus == \"1\") %&gt;% \n  ggplot()+\n  aes(LateralDeviation, \n      ARTCC)+\n  xlab(NULL)+\n  geom_jitter(col = \"grey\", size = .5, alpha = 0.2, show.legend = F)+\n  theme_light()\n\nls2 &lt;- sel1 %&gt;% \n  filter(LateralStatus == \"2\") %&gt;% \n  ggplot()+\n  aes(LateralDeviation, \n      ARTCC) +\n  ylab(NULL)+xlab(NULL)+\n  geom_jitter(col = \"red\", size = .5, alpha = 0.2, show.legend = F)+\n  theme_light()\n\nls3 &lt;- sel1 %&gt;% \n  filter(LateralStatus == \"3\") %&gt;% \n  ggplot()+\n  aes(LateralDeviation, \n      ARTCC)+xlab(NULL)+\n  geom_jitter(col = \"green\", size = .5, alpha = 0.2, show.legend = F)+\n  theme_light()\n\nls4 &lt;- sel1 %&gt;% \n  filter(LateralStatus == \"4\") %&gt;% \n  ggplot()+\n  aes(LateralDeviation, \n      ARTCC)+\n  ylab(NULL)+xlab(NULL)+\n  geom_jitter(col = \"blue\", size = .5, alpha = 0.2, show.legend = F)+\n  theme_light()\n\nls5 &lt;- sel1 %&gt;% \n  filter(LateralStatus == \"5\") %&gt;% \n  ggplot()+\n  aes(LateralDeviation, \n      ARTCC)+\n  geom_jitter(col = \"cyan\", size = .5, alpha = 0.2, show.legend = F)+\n  theme_light()\n\nls6 &lt;- sel1 %&gt;% \n  ggplot()+\n  aes(LateralDeviation, \n      ARTCC, col = sel1$LateralStatus)+\n  ylab(NULL)+\n  geom_jitter(col = sel1$LateralStatus, size = .5, alpha = 0.1, show.legend = T)+\n  theme_light()\n\ngrid.arrange(arrangeGrob(ls1, ls2, ls3, ls4, ls5, ls6), nrow=1)\n\n\n\n\n\n\n\nNote: One key insight, the above graph shows ZMA as the control center with the highest variability or dispersion. The trait makes sense considering that ZMA (Miami) sits in a significant different location more susceptible to different weather elements and offshore inbound and transient air traffic.\n\nssel3 &lt;- sel1\n\n# function to calculate bounds\ncalc_iqr_bounds &lt;- function(data) {\n  Q &lt;- quantile(data, probs = c(0.25, 0.75), na.rm = TRUE)\n  iqr &lt;- IQR(data, na.rm = TRUE)\n  lower &lt;- Q[1] - 1.5 * iqr\n  upper &lt;- Q[2] + 1.5 * iqr\n  return(list(lower = lower, upper = upper))\n}\n\n# calculation of bounds\nbounds &lt;- calc_iqr_bounds(ssel3$LateralDeviation)\nNOutliers &lt;- subset(ssel3$LateralDeviation, ssel3$LateralDeviation &gt; bounds$lower & \n                    ssel3$LateralDeviation &lt; bounds$upper)\n\n# distributions with and without outliers \npar(mfrow = c(1, 2))\nboxplot(ssel3$LateralDeviation, col = \"darkgrey\", border = \"darkred\", \n        horizontal = FALSE, main = \"Distribution with Outliers\")\n\nboxplot(NOutliers, col = \"darkgrey\", border = \"darkblue\", \n        main = \"Distribution without Outliers\")\n\n\n\n\n\n\n\nNote: For best perception of these deviations, I went ahead and calculated additional measures applying moving average filters like weighted moving average with a backward window of two positions, the absolute values of the weighted moving average, and other backward windows to include the 4th prior period and the 7th period.\n\npacman::p_load(forecast, pracma)\ndf1 &lt;- ssel3\ndf1[\"WMA\"] &lt;- pracma::movavg(df1$LateralDeviation, n = 2, type = \"w\")\ndf1[\"Abs_WMA\"] &lt;- abs(df1$WMA)\ndf1[\"ma4_lateraldev\"] &lt;- forecast::ma(df1$LateralDeviation, order=4)\ndf1[\"ma7_lateraldev\"] &lt;- forecast::ma(df1$LateralDeviation, order=7)\nsummary(df1)\n\n ARTCC           Airline          UniqueID          MeasureTaken  \n ZAU:549977   SWA    : 190572   Length:1988068     Min.   :61210  \n ZLA:547118   AAL    : 158419   Class :character   1st Qu.:68980  \n ZMA:473121   UAL    : 101846   Mode  :character   Median :74820  \n ZNY:417852   AWE    :  97578                      Mean   :74696  \n              DAL    :  85748                      3rd Qu.:80410  \n              COA    :  73412                      Max.   :86390  \n              (Other):1280493                                     \n xMeasureTaken     xControlStartTm   xControlEndTm     LateralDeviation   \n Length:1988068    Length:1988068    Length:1988068    Min.   :-748.9390  \n Class1:hms        Class1:hms        Class1:hms        1st Qu.:  -0.8658  \n Class2:difftime   Class2:difftime   Class2:difftime   Median :   0.0043  \n Mode  :numeric    Mode  :numeric    Mode  :numeric    Mean   :   0.3468  \n                                                       3rd Qu.:   0.8736  \n                                                       Max.   : 596.9131  \n                                                                          \n CorrectionAngle    LateralStatus      WMA               Abs_WMA        \n Min.   :  0.0000   1:802138      Min.   :-748.7875   Min.   :  0.0000  \n 1st Qu.:  0.8003   2:228148      1st Qu.:  -0.8656   1st Qu.:  0.2012  \n Median :  2.5434   3:161562      Median :   0.0045   Median :  0.8702  \n Mean   : 24.9097   4:711040      Mean   :   0.3468   Mean   :  7.4450  \n 3rd Qu.: 17.2016   5: 85180      3rd Qu.:   0.8747   3rd Qu.:  3.5313  \n Max.   :180.9997                 Max.   : 596.6656   Max.   :748.7875  \n                                                                        \n ma4_lateraldev      ma7_lateraldev     \n Min.   :-747.9141   Min.   :-747.3517  \n 1st Qu.:  -0.8684   1st Qu.:  -0.8775  \n Median :   0.0046   Median :   0.0051  \n Mean   :   0.3467   Mean   :   0.3467  \n 3rd Qu.:   0.8808   3rd Qu.:   0.8919  \n Max.   : 595.2119   Max.   : 594.3059  \n NA's   :4           NA's   :6          \n\n\n\n\nNote: In conclusion of the EDA, I went from having limited understanding of the variables, to a robust and feature-engineered data frame while maintaining the original characteristics of the data. At this point, we want to split the tidy data frame, train and test the classification model.\n\nclean_df1 &lt;- df1[complete.cases(df1),]\nhead(clean_df1)\n\n# A tibble: 6 × 14\n  ARTCC Airline UniqueID    MeasureTaken xMeasureTaken xControlStartTm\n  &lt;fct&gt; &lt;fct&gt;   &lt;chr&gt;              &lt;dbl&gt; &lt;time&gt;        &lt;time&gt;         \n1 ZAU   AAL     AAL1016_141        61590 17:06:30      17:06          \n2 ZAU   AAL     AAL1016_141        61600 17:06:40      17:06          \n3 ZAU   AAL     AAL1016_141        61610 17:06:50      17:06          \n4 ZAU   AAL     AAL1016_141        61620 17:07:00      17:06          \n5 ZAU   AAL     AAL1016_141        61630 17:07:10      17:06          \n6 ZAU   AAL     AAL1016_141        61640 17:07:20      17:06          \n# ℹ 8 more variables: xControlEndTm &lt;time&gt;, LateralDeviation &lt;dbl&gt;,\n#   CorrectionAngle &lt;dbl&gt;, LateralStatus &lt;fct&gt;, WMA &lt;dbl&gt;, Abs_WMA &lt;dbl&gt;,\n#   ma4_lateraldev &lt;dbl&gt;, ma7_lateraldev &lt;dbl&gt;\n\n\n\n\n\n\nReferences\n\nPaglione, Mike, Ibrahim Bayraktutar, Greg McDonald, and Jesper Bronsvoort. 2010. “Lateral Intent Error’s Impact on Aircraft Prediction.” Air Traffic Control Quarterly 18 (1): 29–62. https://doi.org/10.2514/atcq.18.1.29."
  },
  {
    "objectID": "posts/time_series/index.html",
    "href": "posts/time_series/index.html",
    "title": "Time Series Analysis - AAPL",
    "section": "",
    "text": "Time Series Analysis\nThe following is a basic time series analysis of Apple, Inc. stock market between July 2019 and July 2020.\n\n\n\nNotice how the limitation of global imports starts to affect the company around Feb 2020. By March of the same year, the company reported loses over $1 Trillion. Nevertheless, in June 10, 2020, Apple became the first U.S. company to reach $1.5 Trillion market cap.\n\n\nCorrelation between external events, closed price, and total volume.\n\n\nConsidering the previous 20 periods we can display the linear trend and forecast estimate.\n\n\nUltimately, all graphics can get organized as an interactive dashboard using Tableau’s capabilities."
  },
  {
    "objectID": "posts/ctree_classifier/index.html",
    "href": "posts/ctree_classifier/index.html",
    "title": "Conditional Inference Trees",
    "section": "",
    "text": "Introduction\nCardiotocograms, also known as CTGs, have been instrumental within clinical medicine for a long time. Obstetricians use these measurements and classifications to obtain detailed information and intelligence about newborns and their mother prior and during labor. In 2018, an article presented through the Journal of Clinical Medicine detailed the practicality of CTG. The same article noted that interpretations of these censorial readings is mainly attributed to the observer; which creates challenges of consistency of interpretations and defies the human naked- eye. Questions like what happens if/when the interpreter misses a key detail, or what could be the meaning of a combination of diagnostic signals, furthermore, what time-sensitive conditions may these measurements expose, requiring immediate actions? These are few examples of concerns posed by the continuous practice of merely optical assessments of a CTG. (Zhao, Zhang, and Deng 2018)\nThe following exploration presents an assessment of CTGs using the conditional inference tree (ctree) model. The same shows how the algorithm expedites and enhances the interpretation of CTG readings while appraising multiple fetal readings simultaneously. Moreover, the study aims to identify potential hidden patters which may require further attention.\nData\nThe analyzed data comes for the UCI Machine Learning Repository(D. Campos 2000), and it consists of measurements of fetal heart rate (FHR) and other important characteristics as identified and recorded within each cardiotocograms. Ultimately, all CTGs were classified by three subject matter experts, and under unanimity, assigned with response-labels based on the fetal state and/or morphological detected patterns. The following is a list of the variables meaning according to the UCI repository:\n\nLB - FHR baseline (beats per minute)\nAC - # of accelerations per second\nFM - # of fetal movements per second\nUC - # of uterine contractions per second\nDL - # of light decelerations per second\nDS - # of severe decelerations per second\nDP - # of prolonged decelerations per second\nASTV - percentage of time with abnormal short term variability\nMSTV - mean value of short term variability\nALTV - percentage of time with abnormal long term variability\nMLTV - mean value of long term variability Width - width of FHR histogram\nMin - minimum of FHR histogram\nMax - Maximum of FHR histogram\nNmax - # of histogram peaks\nNzeros - # of histogram zeros\nMode - histogram mode\nMean - histogram mean\nMedian - histogram median\nVariance - histogram variance\nTendency - histogram tendency\nCLASS - FHR pattern class code (1 to 10)\nNSP - fetal state class code (N=normal; S=suspect; P=pathologic)\nExploratory Data Analysis\nDuring exploratory data analysis the data is confirmed as a combination of 2126 observations and 23 variables. The following is a preview of the first six observations after been ingested as as_tibble.\n\ndf&lt;-as_tibble(read.csv(file=\"cardiotocography.csv\", head=TRUE, sep=\",\", as.is=FALSE))\nprint(df, n=6)\n\n# A tibble: 2,126 × 23\n     LB    AC    FM    UC    DL    DS    DP  ASTV  MSTV  ALTV  MLTV Width   Min\n  &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt;\n1   120 0         0 0     0         0 0        73   0.5    43   2.4    64    62\n2   132 0.006     0 0.006 0.003     0 0        17   2.1     0  10.4   130    68\n3   133 0.003     0 0.008 0.003     0 0        16   2.1     0  13.4   130    68\n4   134 0.003     0 0.008 0.003     0 0        16   2.4     0  23     117    53\n5   132 0.007     0 0.008 0         0 0        16   2.4     0  19.9   117    53\n6   134 0.001     0 0.01  0.009     0 0.002    26   5.9     0   0     150    50\n# ℹ 2,120 more rows\n# ℹ 10 more variables: Max &lt;int&gt;, Nmax &lt;int&gt;, Nzeros &lt;int&gt;, Mode &lt;int&gt;,\n#   Mean &lt;int&gt;, Median &lt;int&gt;, Variance &lt;int&gt;, Tendency &lt;int&gt;, CLASS &lt;int&gt;,\n#   NSP &lt;int&gt;\n\n\nThe following code chunks portray a basic assessment of specific attributes and areas of importance such as variability of observations, presence of missing values, mean, standard deviation,\n\n# How much variability the main predictor shows? \nlbx &lt;- IQR(df$LB)\nsummary(df$LB)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  106.0   126.0   133.0   133.3   140.0   160.0 \n\n\nNote: LB attribute’s IQR equals 14, which is significantly small indicating a most values to be clustered around the middle. The following histogram confirms the small IQR.\n\nhist(df$LB, breaks = 12, main=\"Histogram of FHR Baseline\", xlab=\"(beats per minute)\",\n     border=\"darkblue\",col =\"lightgrey\", labels = F)\n\n\n\n\n\n\n\n\n# Are there any missing values present?\ncolSums(is.na(df))\n\n      LB       AC       FM       UC       DL       DS       DP     ASTV \n       0        0        0        0        0        0        0        0 \n    MSTV     ALTV     MLTV    Width      Min      Max     Nmax   Nzeros \n       0        0        0        0        0        0        0        0 \n    Mode     Mean   Median Variance Tendency    CLASS      NSP \n       0        0        0        0        0        0        0 \n\n\n\nt.test(df$LB)\n\n\n    One Sample t-test\n\ndata:  df$LB\nt = 624.59, df = 2125, p-value &lt; 2.2e-16\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n 132.8853 133.7224\nsample estimates:\nmean of x \n 133.3039 \n\n\n\n# LB stats\nm&lt;-mean(df$LB)\nstd&lt;-sd(df$LB)\nupr=m+std\nlwr=m-std\nlbdf &lt;- data.frame(df,my_x = 0 + rnorm(length(df$LB),\n        mean=m, sd=std),my_y = 0 + rnorm(length(df$LB), mean=m, sd=std))\n# LB Variation\nprint(pltlb &lt;- ggplot(lbdf, xlab = F, aes(x=(my_x), y=my_y)) + \n        geom_line(col=\"grey51\",linemitre=1) +\n        geom_smooth(method=lm , color=\"blue\", lty=3, fill=\"light blue\", se=T) +\n        labs(x=NULL, y=\"BPM\", title=\"FHR LB Variation\\nIn Relation To The Mean\")+\n        theme_ipsum())\n\n\n\n\n\n\n\n\n# very first graph representation with manual boundary calculations\nupr2=m+(std*2)\nlwr2=m-(std*2)\n# Plot LB distribution boundaries \nplot.new()\nplot(df$LB, type=\"l\", col=\"grey51\", ylab=\"LB\", main=\"1 & 2 Standard Deviations\")\nabline(h = m, col = \"blue\")\nabline(h = upr, col = \"orange\", lty=2)\nabline(h = lwr, col = \"orange\", lty=2)\nabline(h = upr2, col = \"red\", lty=2)\nabline(h = lwr2, col = \"red\", lty=2)\ntext(-65,134, \"mean:133.30\", col = \"blue\", adj = c(0, -.1))\ntext(-65,upr, round(upr, 2), col = \"black\", adj = c(0, -.1))\ntext(-65,lwr, round(lwr, 2), col = \"black\", adj = c(0, -.1))\ntext(-65,upr2, round(upr2, 2), col = \"black\", adj = c(0, -.1))\ntext(-65,lwr2, round(lwr2, 2), col = \"black\", adj = c(0, -.1))\n\n\n\n\n\n\n# LB Observations higher than 2-s.d.\n lba&lt;-(sum(df$LB &gt;152.99)) #39\n# LB Observations lower than 2-s.d.\n lbb&lt;-(sum(df$LB &lt;113.62)) #44\n lba+lbb #=83 obs outside of 2-s.d.\n\n[1] 83\n\nsum(between(df$LB, 113.62, 152.99))/nrow(df) # of obs within 2-s.d.\n\n[1] 0.9609595\n\n\n\n# Exclude non-original measurements, rename targeted values\ndf[12:22] &lt;- NULL\ndf$NSP&lt;-as.numeric(df$NSP)\n# enumeration of labels with the factor function\ndf$NSP&lt;-factor(df$NSP, levels= 1:3, labels = c(\"Normal\",\"Suspect\", \"Pathologic\"))\n\n\n# Visualization of original NSP\nplot(df$NSP, main=\"Original NSP Distribution\",\n     xlab=\"Fetal State Classification\", \n     ylab=\"Frequency\", col=c(3, 7, 2))\ntext(df$NSP, labels=as.character(tabulate(df$NSP)), adj=3, pos=3)\n\n\n\n\n\n\n\n\n# additional way to preview distribution of attributes\n# distributions preview\ndf[,1:12] %&gt;%\n  gather() %&gt;%\n  ggplot(aes(value)) +\n  theme_light() + labs( title=\"FHR Measurement Distributions\")+\n  theme(axis.text.x = element_text(angle=90)) +\n  facet_wrap(~ key, scales = \"free\", shrink = TRUE) +\n  geom_bar(mapping = aes(value),\n           color=\"darkblue\", fill=\"lightgrey\")\n\n\n\n\n\n\n\nIn progress …\n\n# Summary of DF after encoding the label vector as numbers. \nsummary(df)\n\n       LB              AC                 FM                 UC          \n Min.   :106.0   Min.   :0.000000   Min.   :0.000000   Min.   :0.000000  \n 1st Qu.:126.0   1st Qu.:0.000000   1st Qu.:0.000000   1st Qu.:0.002000  \n Median :133.0   Median :0.002000   Median :0.000000   Median :0.004000  \n Mean   :133.3   Mean   :0.003178   Mean   :0.009481   Mean   :0.004366  \n 3rd Qu.:140.0   3rd Qu.:0.006000   3rd Qu.:0.003000   3rd Qu.:0.007000  \n Max.   :160.0   Max.   :0.019000   Max.   :0.481000   Max.   :0.015000  \n       DL                 DS                  DP                 ASTV      \n Min.   :0.000000   Min.   :0.000e+00   Min.   :0.0000000   Min.   :12.00  \n 1st Qu.:0.000000   1st Qu.:0.000e+00   1st Qu.:0.0000000   1st Qu.:32.00  \n Median :0.000000   Median :0.000e+00   Median :0.0000000   Median :49.00  \n Mean   :0.001889   Mean   :3.293e-06   Mean   :0.0001585   Mean   :46.99  \n 3rd Qu.:0.003000   3rd Qu.:0.000e+00   3rd Qu.:0.0000000   3rd Qu.:61.00  \n Max.   :0.015000   Max.   :1.000e-03   Max.   :0.0050000   Max.   :87.00  \n      MSTV            ALTV             MLTV                NSP      \n Min.   :0.200   Min.   : 0.000   Min.   : 0.000   Normal    :1655  \n 1st Qu.:0.700   1st Qu.: 0.000   1st Qu.: 4.600   Suspect   : 295  \n Median :1.200   Median : 0.000   Median : 7.400   Pathologic: 176  \n Mean   :1.333   Mean   : 9.847   Mean   : 8.188                    \n 3rd Qu.:1.700   3rd Qu.:11.000   3rd Qu.:10.800                    \n Max.   :7.000   Max.   :91.000   Max.   :50.700                    \n\n\n\n# split the data into a training and test sets\nset.seed(1234)\nind &lt;- sample(2, nrow(df), replace = T, prob = c(0.70, 0.30))\ntrain.data &lt;- df[ind == 1, ]\ntest.data &lt;- df[ind == 2, ]\n\n#run the method on a training data\nmyFormula&lt;-NSP~.\nmodel &lt;- ctree(myFormula, data = train.data)\n\n\n# output the tree structure\n# print(model)\nmodel[4]\n\n\nModel formula:\nNSP ~ LB + AC + FM + UC + DL + DS + DP + ASTV + MSTV + ALTV + \n    MLTV\n\nFitted party:\n[4] root\n|   [5] ASTV &lt;= 73\n|   |   [6] DL &lt;= 0.008\n|   |   |   [7] DP &lt;= 0\n|   |   |   |   [8] LB &lt;= 149\n|   |   |   |   |   [9] AC &lt;= 0.001\n|   |   |   |   |   |   [10] UC &lt;= 0: Normal (n = 34, err = 14.7%)\n|   |   |   |   |   |   [11] UC &gt; 0: Normal (n = 231, err = 3.5%)\n|   |   |   |   |   [12] AC &gt; 0.001: Normal (n = 626, err = 0.3%)\n|   |   |   |   [13] LB &gt; 149: Normal (n = 17, err = 35.3%)\n|   |   |   [14] DP &gt; 0\n|   |   |   |   [15] MLTV &lt;= 0.9: Normal (n = 7, err = 28.6%)\n|   |   |   |   [16] MLTV &gt; 0.9\n|   |   |   |   |   [17] MLTV &lt;= 8.8: Normal (n = 28, err = 0.0%)\n|   |   |   |   |   [18] MLTV &gt; 8.8: Normal (n = 7, err = 28.6%)\n|   |   [19] DL &gt; 0.008\n|   |   |   [20] ASTV &lt;= 58: Normal (n = 35, err = 0.0%)\n|   |   |   [21] ASTV &gt; 58: Pathologic (n = 25, err = 40.0%)\n|   [22] ASTV &gt; 73: Pathologic (n = 11, err = 45.5%)\n\nNumber of inner nodes:     9\nNumber of terminal nodes: 10\n\n\n\n#8. visualize the tree\n# plot(model, main=\"Cardiotocography Data\\n Conditional Inference Tree\\n'Extended'\",\n#       type=\"simple\",ep_args = list(justmin = 8), drop_terminal = F, \n#      gp = gpar(fontsize = 9), margins = c(4,4, 4, 4))\n\nplot(model, type=\"extended\", ep_args = list(justmin =8), drop_terminal=F, tnex=1.5, \n     gp=gpar(fontsize = 8, col=\"dark blue\"),\n     inner_panel = node_inner(model, fill=c(\"light grey\",\"cyan\"), pval=T), \n     terminal_panel=node_barplot(model, fill=c(3,7,2), beside=T, ymax=1, rot = 45, \n     just = c(.95,.5), ylines=F, widths = 1, gap=0.05, reverse=F, id=T), \n     margins = c(3,0, 3, 0),\n     main =\"Cardiotocography Data\\n Conditional Inference Tree\\n'Extended'\")\n\n\n\n\n\n\n\n\n#9. Confusion matrix\ntable(predict(model), train.data$NSP, dnn=c(\"PREDICTED\", \"ACTUAL\"))\n\n            ACTUAL\nPREDICTED    Normal Suspect Pathologic\n  Normal       1168      70          4\n  Suspect        10     123          1\n  Pathologic     17       8        122\n\n# predicted classification accuracy with training data\nsum(predict(model) == train.data$NSP)/length(train.data$NSP)\n\n[1] 0.9277741\n\nprop.table(table(predict(model), train.data$NSP, dnn=c(\"PREDICTED\", \"ACTUAL\")))\n\n            ACTUAL\nPREDICTED          Normal      Suspect   Pathologic\n  Normal     0.7669074196 0.0459619173 0.0026263953\n  Suspect    0.0065659882 0.0807616546 0.0006565988\n  Pathologic 0.0111621799 0.0052527905 0.0801050558\n\n\n\n#10. Evaluate the model on a test data\nmodel2 &lt;- ctree(myFormula, data = test.data)\nmodel2[4]\n\n\nModel formula:\nNSP ~ LB + AC + FM + UC + DL + DS + DP + ASTV + MSTV + ALTV + \n    MLTV\n\nFitted party:\n[4] root\n|   [5] DP &lt;= 0\n|   |   [6] DL &lt;= 0.01\n|   |   |   [7] ALTV &lt;= 1: Normal (n = 291, err = 1.0%)\n|   |   |   [8] ALTV &gt; 1\n|   |   |   |   [9] LB &lt;= 143\n|   |   |   |   |   [10] ASTV &lt;= 59: Normal (n = 78, err = 0.0%)\n|   |   |   |   |   [11] ASTV &gt; 59: Normal (n = 14, err = 42.9%)\n|   |   |   |   [12] LB &gt; 143\n|   |   |   |   |   [13] ASTV &lt;= 45: Normal (n = 7, err = 14.3%)\n|   |   |   |   |   [14] ASTV &gt; 45: Suspect (n = 13, err = 15.4%)\n|   |   [15] DL &gt; 0.01: Normal (n = 10, err = 30.0%)\n|   [16] DP &gt; 0\n|   |   [17] MSTV &lt;= 1.7: Normal (n = 12, err = 25.0%)\n|   |   [18] MSTV &gt; 1.7: Normal (n = 12, err = 50.0%)\n\nNumber of inner nodes:    7\nNumber of terminal nodes: 8\n\n\n\n# plot(model2, main=\"Cardiotocography Data\\n Simple Conditional Inference Tree\\nby ocardec\",\n#      type=\"simple\",ep_args = list(justmin = 10), drop_terminal = F, gp = gpar(fontsize = 12))\n\nplot(model2, ep_args = list(justmin = 8), type=\"extended\", drop_terminal = F, \n     tnex=1, gp= gpar(fontsize = 8, col=\"dark blue\"), \n     inner_panel = node_inner (model2, fill=c(\"lightgrey\",\"yellow\"), pval=T, id=T),\n     terminal_panel=node_barplot(model2, col=\"black\", fill=c(3,7,2, 0.3), beside=T, \n     ymax=1, rot = 45, just = c(\"right\", \"top\"), ylines=F, \n     widths=1, gap=0.1, reverse=F, id=F), margins = c(3, 0, 3, 0), \n     main=\"Cardiotocography Data\\n Extended Conditional Inference Tree\\nby ocardec\")\n\n\n\n\n\n\n\n\n# Confusion matrix and stats\ntestPred2 &lt;- predict(model2, newdata = test.data, method=\"NSP\")\nconfusionMatrix(testPred2, test.data$NSP)\n\nConfusion Matrix and Statistics\n\n            Reference\nPrediction   Normal Suspect Pathologic\n  Normal        449      21         12\n  Suspect         9      73          4\n  Pathologic      2       0         33\n\nOverall Statistics\n                                          \n               Accuracy : 0.9204          \n                 95% CI : (0.8958, 0.9407)\n    No Information Rate : 0.7629          \n    P-Value [Acc &gt; NIR] : &lt; 2.2e-16       \n                                          \n                  Kappa : 0.7809          \n                                          \n Mcnemar's Test P-Value : 0.001165        \n\nStatistics by Class:\n\n                     Class: Normal Class: Suspect Class: Pathologic\nSensitivity                 0.9761         0.7766           0.67347\nSpecificity                 0.7692         0.9745           0.99639\nPos Pred Value              0.9315         0.8488           0.94286\nNeg Pred Value              0.9091         0.9594           0.97183\nPrevalence                  0.7629         0.1559           0.08126\nDetection Rate              0.7446         0.1211           0.05473\nDetection Prevalence        0.7993         0.1426           0.05804\nBalanced Accuracy           0.8727         0.8755           0.83493\n\n\n\n\n\nReferences\n\nD. Campos, J. Bernardes. 2000. “Cardiotocography.” UCI Machine Learning Repository. https://doi.org/10.24432/C51S4N.\n\n\nZhao, Zhidong, Yang Zhang, and Yanjun Deng. 2018. “A Comprehensive Feature Analysis of the Fetal Heart Rate Signal for the Intelligent Assessment of Fetal State.” Journal of Clinical Medicine 7 (8): 223. https://doi.org/10.3390/jcm7080223."
  },
  {
    "objectID": "posts/multi_model_part2/index.html",
    "href": "posts/multi_model_part2/index.html",
    "title": "Aicraft’s Lateral Deviation - Part 2",
    "section": "",
    "text": "Introduction\nAs previously explained on Aircraft’s Lateral Deviation - Part I, understanding of an aircraft lateral intent is vital across the air traffic industry for safety, and flight efficiency among other factors. This task can easily become unwieldy when triaging millions of measurements per hour across the entire National Air Space (NAS). On this section, I present a variety of potential predictive models for the classification of an aircraft’s lateral intent, and which could improve automated detection systems across this field.\n\nDisclaimer: The views and opinions expressed in this report are those of the author and do not necessarily reflect the views or positions of any of the entities herein referred.\n\nOriginal Data\nThe first step was to ingest the tidy data generated in Part I. Note, as showing below the data class label with the least number of observations is “EndOfRoute” with ~85,000; indicating a potential option to downsample the data before fitting the models.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\n\nInConf\nN = 802,138\n1\n\n\nMid-InConf\nN = 228,148\n1\n\n\nMid-NonConf\nN = 161,561\n1\n\n\nOutConf\nN = 711,038\n1\n\n\nEndOfRoute\nN = 85,177\n1\n\n\np-value\n2\n\n\n\n\nLateralDeviation\n0 (0, 0)\n1 (-1, 1)\n1 (-1, 1)\n-2 (-5, 4)\n1 (-14, 17)\n&lt;0.001\n\n\nCorrectionAngle\n1 (1, 3)\n2 (1, 5)\n3 (1, 17)\n13 (2, 52)\n181 (181, 181)\n&lt;0.001\n\n\nWMA\n0 (0, 0)\n0 (-1, 1)\n0 (-1, 1)\n-1 (-5, 4)\n1 (-14, 17)\n&lt;0.001\n\n\nAbs_WMA\n0 (0, 0)\n1 (1, 1)\n1 (1, 1)\n4 (2, 11)\n15 (7, 28)\n&lt;0.001\n\n\nma4_lateraldev\n0 (0, 0)\n0 (-1, 1)\n0 (-1, 1)\n-1 (-5, 4)\n1 (-14, 16)\n&lt;0.001\n\n\n\n\n\n1\nMedian (Q1, Q3)\n\n\n\n\n2\nKruskal-Wallis rank sum test\n\n\n\n\n\n\n\n\nScatter Plot of Matrices (SPLOM)\nPre-processing\nBefore proceeding to model training, I took three specific actions to ensure the proper conditioning of the data. First, I scaled the numerical attributes. The main objective of scaling the data was to address skewedness and data outliers. Second, I normalized the data to enhance the model performance. Normalization is critical to prevent any attributes with larger scales to dominates the model learning process. Lastly, I finished conditioning the data by downsampling it so all classes would end up with the same frequency as the minority class.\n\ndf1[c(1:3)] &lt;- scale(df1[c(1:3)])\ndf1 &lt;- normalize(df1)\n\n# set seed\nset.seed(1212)\n# down-sample entire data set\ndf2 &lt;- downSample(x=df1[c(-4)], y=df1$Class)\n\nFor duplication purposes, I utilized 10% of the conditioned data.\n\nsummary(sampled_df)\n\n LateralDeviation   CorrectionAngle          WMA                  Class     \n Min.   :0.009301   Min.   :0.0000002   Min.   :0.00932   InConf     :8517  \n 1st Qu.:0.555604   1st Qu.:0.0056313   1st Qu.:0.55566   Mid-InConf :8517  \n Median :0.556482   Median :0.0267311   Median :0.55653   Mid-NonConf:8517  \n Mean   :0.556627   Mean   :0.2676967   Mean   :0.55667   OutConf    :8517  \n 3rd Qu.:0.557358   3rd Qu.:0.3721161   3rd Qu.:0.55741   EndOfRoute :8517  \n Max.   :0.935658   Max.   :1.0000000   Max.   :0.93549                     \n\n\nData Partitioning\nPost-conditioning of the data, I applied a train-test split method in preparation to properly evaluate the performance of the models. The idea here is to divide the dataset into three distinctive subsets, training, testing, and cross-validating, and observe how the model generalize across these.\n\n# to create a 50-30-20 subsets\nset.seed(12345)\n# training set with 50% of the data\ntrainIndex &lt;- createDataPartition(sampled_df$Class, p = 0.50, list = FALSE)\ntrain &lt;- sampled_df[trainIndex, ]\n# remaining 50%\nremaining &lt;- sampled_df[-trainIndex, ]\n# test set (30%)\ntestIndex &lt;- createDataPartition(remaining$Class, p = 0.60, list = FALSE)\ntest &lt;- remaining[testIndex, ]\n# cross-validation (20%)\ncrossval &lt;- remaining[-testIndex, ]\n\n# preview of the partitions\ntable(train$Class)\n\n\n     InConf  Mid-InConf Mid-NonConf     OutConf  EndOfRoute \n       4259        4259        4259        4259        4259 \n\ntable(test$Class)\n\n\n     InConf  Mid-InConf Mid-NonConf     OutConf  EndOfRoute \n       2555        2555        2555        2555        2555 \n\ntable(crossval$Class)\n\n\n     InConf  Mid-InConf Mid-NonConf     OutConf  EndOfRoute \n       1703        1703        1703        1703        1703 \n\n\n\nClassification Models\n\nConditional Inference Tree\nOne of the implemented classification models was a conditional inference tree (CTREE). The CTREE method follows a “recursive partitioning framework” to split each evaluated class and “the outcome takes place based on the measured p-value of association between the observations” at hand. As an example of the steps taken for most of the models, here is a copy of the code and graphic.\n\nset.seed(12345) \nmyFormula &lt;- Class ~. \n# start.time &lt;- Sys.time()\nmodel &lt;- partykit::ctree(myFormula, control=ctree_control(maxdepth= ), data=train)\n# end.time &lt;- Sys.time()\n# time.taken &lt;- end.time - start.time\n\n# train model\n# table(predict(model), train$Class, dnn=c(\"PREDICTED\", \"ACTUAL\"))\nmodelaccur &lt;- sum(predict(model) == train$Class) / length(train$Class) \n# prop.table(table(predict(model), train$Class, dnn=c(\"PREDICTED\", \"ACTUAL\")))\n\n# test model\nmodel2 &lt;- ctree(myFormula, data = test)\n# table (testPred2, test$Class, dnn=c(\"PREDICTED\", \"ACTUAL\")) \ntestPred2 &lt;- predict(model2, data = test, method=\"class\") \nmodel2accur &lt;- sum(testPred2 == test$Class)/length(test$Class)\n# prop.table(table(predict(model2), test$Class, dnn=c(\"PREDICTED\", \"ACTUAL\")))\n\n# cross-val model\nmodel3 &lt;- ctree(myFormula, data = crossval)\n# table (testPred3, crossval$Class, dnn=c(\"PREDICTED\", \"ACTUAL\"))\ntestPred3 &lt;- predict(model3, data = crossval, method=\"class\") \nmodel3accur &lt;- sum(testPred3 == crossval$Class)/length(crossval$Class)\n# prop.table(table(predict(model3), crossval$Class, dnn=c(\"PREDICTED\", \"ACTUAL\")))\n\nFor the CTREE depiction I used the partykit::ctree R package(Hothorn and Zeileis 2015). Here’s a glimpse of how the cross-validation model performs and shows the distribution of classes in the terminal nodes. The graphic greatly helps conceptualizing the model and provides a clearer image of the lateral deviations and status changes relationship.\n\nplot(model3, type=\"extended\", ep_args = list(justmin = .01),\n     drop_terminal=T, tnex= 1.3, gp=gpar(fontsize = 8, col=\"darkblue\"),\n     inner_panel = node_inner(model, fill=c(\"white\",\"green\"), pval=T),\n     terminal_panel= node_barplot(model, fill=c(1:5), beside=T,\n                                  ymax=1.05, rot = 90, just = c(.95,.5),\n                                  ylines=T, widths = .90, gap=0.05, reverse=F, id=T),\n     margins = c(6,5, 5, 4), main =\"Conditional Inference Tree\\nLateral Adherence Status\")\n\n\n\n\n\n\n\n\nNaïve Bayes\nThe second classification model –Naïve Bayes (NB)—follows Thomas Bayes’ theorem of probabilistic classification judging the relationship between the probabilities of different events and their conditional probabilities. Per this theorem, the data is used to fit the model, assuming “the encompassing predictors are independent of the target variable classes”(Majka 2024). One note about NB was the capacity of the model to evaluate all the instances in a swiftly 1.60 seconds.\n\n\nMulticlass Support Vector Machine (SVM)\nThe third model, a SVM was adapted using the one-versus-one approach, “C-classification” type, to assess the multiple classes within the observations. The kernel used for the calculation was a “radial”, the data was scaled, and the cost and gamma values were kept at default values with 10 and 0.1 respectively. It resulted in identifying 3,814 support vectors, reaching an overall accuracy, recall, precision, and F1 of 97%\n\n\nGradient Boosting Machine (GBM)\nThe fourth predicted model is an adoption of the typical ensemble model of generalized boosted regression modeling known as Stochastic Gradient Boosting Machine. This model is particularly “known as one of the most powerful techniques for building predictive models”. Reportedly, GBM scopes optimal performance when dealing with significant disparity across observations and it is highly recommended to use it for anomaly detection cases. In essence, the GBM framework takes the error or loss, of the prior and employs it to adjust the weights of the sub-sequential trees, thus minimizing the follow-on errors.\n\n\n\n\n\nModels Review\nAt the end, all models were compared and evaluated for best performance. Although, all the models performed relatively well, there were unique traits to arrange these models in three groups. Case in point, top performers based on overall accuracy were the GBM, CTREE, and SVM-M. The second group included NB, and K-Nearest Neighbor (KNN). Finally, the third group with Recursive Partitioning and Regression Tree (RPART). The bellow image displays a comparison of models based on accuracy, and alternatively, the models’ classification error metrics.\n\n\nOverall Performance Metrics\nOverall, four key metrics were taken in consideration, specificity, sensitivity, kappa, and F1 score. As displayed, the classification models graph portray how each model performed, underlining different alternatives to further develop in support of the main objective of auto-classifying in-flight aircraft lateral deviations. The champion in this occasion was GBM with the highest probabilities across the metrics.\n\n\nSummary and Recommendations\nEssentially the project exposed a variety of features of importance regarding the examination of an aircraft lateral variability. In summary, deviation across some ARTCCs like ZLA and ZMA were more prevalent than others, particularly the ZAU or ZNY case. Also, the study demonstrated that the majority of the observations were within three standard deviations from the mean, indicating a consistency of data points across the distributions. Such insights were instrumental explaining inferences from the analysis across the large population. Moreover, the study validated the correlation and relative influence of the correction angle attribute [originally named Angle2NextFix] and the absolute weighted moving average of the lateral deviation [known as redLat] as a principal component or driver in the classification of the different lateral statuses.\nAs mentioned, the completion of this assessment highlights some successes and potential benefits to consider in order to improve the FAA’s decision support tools. However, the shallow level of the study is implicit and further exploration of the data, and validation, is highly advisable and/or testing of the models against operational data. Additional features to correlate could add value to a final proposition and utility of the assessment. Also, other data like GPS data, lat/log coordinates, weather data, approved flight plan details or supplemental sensor measurements could maximize new insights.\n\n\n\nReferences\n\nHothorn, Torsten, and Achim Zeileis. 2015. “Partykit: A Modular Toolkit for Recursive Partytioning in r” 16: 3905–9. https://jmlr.org/papers/v16/hothorn15a.html.\n\n\nMajka, Michal. 2024. “Naivebayes: High Performance Implementation of the Naive Bayes Algorithm in r.” https://CRAN.R-project.org/package=naivebayes."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Projects",
    "section": "",
    "text": "Welcome to my projects!\n\nThis is my first post, it’s my hope you enjoy the content of this portfolio and learn something new from my posts. Please reach back and feel free to provide some constructive feedback.\nLet’s connect."
  },
  {
    "objectID": "posts/geo_analysis_infograph/index.html",
    "href": "posts/geo_analysis_infograph/index.html",
    "title": "Geospatial Analysis Infographic",
    "section": "",
    "text": "Geospatial Infographic\nThe main objective of this visual is to explore relationships and potential patterns between economic prosperity and possible quality of life across the included countries. This could be the beginning of deeper assessments to contain: economic disparities, policy and decision making, international relationships, health care, income distribution, and more.\n\n\n\n\n\n\nData source: World Bank Group"
  },
  {
    "objectID": "posts/join_alliance_infograph/index.html",
    "href": "posts/join_alliance_infograph/index.html",
    "title": "Join the Alliance",
    "section": "",
    "text": "Infographic\nBy definition, an infographic is a visual representation of information, data, or knowledge intended to present complex information, quickly, and clearly. It combines graphics, text, and data to communicate a message, explain a concept or show data patterns and relationships.\n\n\n\n\n\n\nData source: Kaggle"
  },
  {
    "objectID": "posts/breast_cancer_detection/index.html",
    "href": "posts/breast_cancer_detection/index.html",
    "title": "Cancerous Cells Classification - Neural Network",
    "section": "",
    "text": "Introduction\nBreast cancer is “the most common cause of cancer deaths among women worldwide”. In the United States, breast cancer is second to lung cancer related deaths, making it a national health critical issue. Statistical facts show breast cancer as the most frequently diagnosed cancer in women in 140 out of 184 countries. Key to survival and remission of breast cancer is closely linked with early detection and intervention.(Henderson 2015).\nEarly signs of irregular cells growth are detected by sampling and analyzing nuclear changes and parameter using diagnostic tools. The results of these nuclear morphometry tests are evaluated for structural deviations, which are representative of cancer diagnosis.(Narasimha, Vasavi, and Harendra Kumar 2013) Now, considering the significance in accuracy of these evaluations, one may question how the medical industry uses machine learning models like neural networks to augment diagnosis and judgement of such vital medical assessments.\nThe following is post-study report of numerous breast cancer preventive screenings, scrutinizing cell nuclei parameters in order to classify the specimens as either malignant or benign. The data have been previously categorized, thus, the intent here is to employ a neural network methodology to replicate this categorization and measure the algorithm’s effectiveness supporting medical professionals in the identification and early detection of breast carcinoma.\n\n\n\n\nData\nThe employed data, Breast Cancer Wisconsin - Diagnostic,comes from the UCI Machine Learning Repository. The multivariate set contains 569 instances and 32 attributes as described bellow. As previously stated, the data set features quantitative observations representative of the images obtained by means of a fine needle aspirate (FNA). These digitized samples were studied, measured and recorded, ultimately enabling the classification of every instance as malignant or benign. Essentially, there is a total of 10 real-value features per cell, however, given the 3-dimensional fragmentation of each cell sampling, it produces a total of 30 observations across 3 planes.(William Wolberg 1993)\n\nVariables list across each plane\n\n1 - ID Name :: Identification number of the sample\n2 - Diagnosis :: Dependable variable label. M = Malignant, B = Benignant\n3 - Feature_1 Radius :: Mean of distances from center to points on the perimeter\n4 - Feature_2 Texture :: Standard deviation of gray-scale values\n5 - Feature_3 Perimeter ::\n6 - Feature_4 Area ::\n7 - Feature_5 Smoothness :: Local variation in radius lengths\n8 - Feature_6 Compactness :: Perimeter^2 / Area - 1.0\n9 - Feature_7 Concavity :: Severity of concave portions of the contour\n10 - Feature_8 Concave Points :: Number of concave portions of the contour\n11 - Feature_9 Symmetry ::\n12 - Feature_10 Fractal Dimension :: Coastline approximation - 1\n\n\nExploratory Data Analysis\n\nBasic descriptive statistics of the data set.\n\n\n# sample of statistical summary - Plane 1\ndescribe(df[1:10], interp = TRUE, ranges = FALSE)\n\n           vars   n   mean     sd skew kurtosis    se\nDiagnosis*    1 569   1.37   0.48 0.53    -1.73  0.02\nFeature_1     2 569  14.13   3.52 0.94     0.81  0.15\nFeature_2     3 569  19.29   4.30 0.65     0.73  0.18\nFeature_3     4 569  91.97  24.30 0.99     0.94  1.02\nFeature_4     5 569 654.89 351.91 1.64     3.59 14.75\nFeature_5     6 569   0.10   0.01 0.45     0.82  0.00\nFeature_6     7 569   0.10   0.05 1.18     1.61  0.00\nFeature_7     8 569   0.09   0.08 1.39     1.95  0.00\nFeature_8     9 569   0.05   0.04 1.17     1.03  0.00\nFeature_9    10 569   0.18   0.03 0.72     1.25  0.00\n\n\n\nDensity exploration of diagnosis across four specific variables: radius, area, texture, and concave\n\n\n# Explore density \ng1 &lt;- ggplot(data = df)+\n  theme_minimal()+\n  geom_density(mapping = aes(Feature_1,  fill = Diagnosis), col=\"darkgrey\", show.legend = FALSE, alpha=0.5)+ \n  labs(title = \"Density of Diagnosis per Attribute | Red = Benignant, Blue = Malignant\", y = \" \", x = \"Radius\")\n\ng2 &lt;- ggplot(data = df)+\n  theme_minimal()+\n  geom_density(mapping = aes(Feature_2, fill = Diagnosis), col=\"darkgrey\", show.legend = FALSE, alpha=0.5)+ \n  labs(title = \"\", y = \" \", x = \"Texture\")\n\ng3 &lt;- ggplot(data = df)+\n  theme_minimal()+\n  geom_density(mapping = aes(Feature_4, fill = Diagnosis), col=\"darkgrey\", show.legend = FALSE, alpha=0.5)+ \n  labs(title = \"\", y = \" \", x = \"Area\")\n\ng4 &lt;- ggplot(data = df)+\n  theme_minimal()+\n  geom_density(mapping = aes(Feature_7, fill = Diagnosis), col=\"darkgrey\", show.legend = FALSE, alpha=0.5)+ \n  labs(title = \"\", y = \" \", x = \"Concavity\")\n\ngrid.arrange(arrangeGrob(g1,  g2,  g3, g4),  nrow=1)\n\n\n\n\n\n\n\nKey insight, these density plots are useful alternatives illustrating continuous data point. The selected variables are just examples of what can swiftly be illustrated to identify potential relationships between the feature and dependent variable.\nPre-Visualization of Data\nHere I use a conditional inference tree to estimate relationships across the data and how its recursively partitioned by the algorithm criteria. The main intent here is to have an idea on what to expect regarding the classification of this data set.\n\n# mutate diagnosis character to numeric\ndf &lt;- df |&gt; \n  mutate(Diagnosis = ifelse(Diagnosis == \"M\", 1, 0)) \n\n# CTREE model and plot\nmodel &lt;- ctree(Diagnosis ~., data = df)\nplot(model, type=\"extended\", ep_args = list(justmin=8), \n     main=\"Breast Cancer | Preliminary Analysis\",\n     drop_terminal=FALSE, tnex=1.5, \n     gp = gpar(fontsize = 12, col=\"darkblue\"),\n     inner_panel = node_inner(model, fill=c(\"white\",\"green\"), pval=TRUE), \n     terminal_panel=node_barplot(model, fill=rev(c(\"darkred\",\"lightgrey\")), beside=TRUE, ymax=1.0, \n                                 just = c(0.95,0.5), ylines=TRUE, widths = 1.0, gap=0.05, \n                                 reverse=FALSE, id=TRUE)\n     )\n\n\n\n\n\n\n\nScatterplot of Matrix (SPLOM)\nThis scatterplox matrix portraits immediate correlation between the included variables and possible multicollinearity among these. The selected features were limited to the first plane only (items 1 to 10). The other two planes include similar features. My immediate take was to consider a method for dimensionality reduction even when considering a neural network classification model.\n\n\n# Plot Plane I\nclrs &lt;- c(\"darkred\", \"lightgrey\")\n\npairs(df[1:11], fill=clrs, main = \"Plane I - Matrix of Scatterplots\", \n      cex.main= 2.0, cex.labels = 1.0, lower.panel = NULL, pch = 21, \n      col=\"grey\", bg = clrs [unclass(df$Diagnosis)])\n\npar (xpd = TRUE)\n\nlegend (0.10, 0.01, horiz = TRUE, as.vector(unique(df$Diagnosis)), fill=clrs, bty = \"n\")\n\n\n\n\n\n\n\nDimensionality Reduction\nAs defined, the purpose of dimensionality reduction is to find a method that can represents a given data set using a smaller number of features but still containing the original data’s properties. I know there are different methods to accomplish this, case in point, LDA, PCa, t-SNE, K-NN, UMAP, etc., but I ultimately decided to use PCA for feature extraction. The following steps illustrate how the method identifies eigenvector of largest eigenvalues of across the covariance matrix. As a result, I create a sub-set of the data using these variables with maximum influence on variance.\n\n# pca of original data\nres.pca &lt;- PCA(df, scale.unit = TRUE, graph = FALSE, ncp = 4)\neig.val &lt;- get_eigenvalue(res.pca)\nvar &lt;- get_pca_var(res.pca)\n\n# Color by cos2 values: quality on the factor map\nfviz_pca_var(res.pca, col.var = \"contrib\",\n             gradient.cols = c(\"#00AFBB\", \"#E7B800\", \"#FC4E07\"), \n             repel = TRUE \n             )\n\n\n\n\n\n\n\nSub-selected Features\nHere’s my source code to create a subset based on the PCA, followed by conditioning the target variable as a factor for down-sampling purposes. The down-sampling approach was ensure an equally number of target outcomes and avoiding any model disposition towards one way or the other. Completed the down-sampling, I scale and centered the data to maximize model performance.\n\n# selection of principal components \npdf &lt;- df |&gt; \n  select(Diagnosis, Feature_1, Feature_2, Feature_3, Feature_4, Feature_6, Feature_7, Feature_8,\n         Feature_10, Feature_11, Feature_13, Feature_14, Feature_16, Feature_18, Feature_20,\n         Feature_21, Feature_23, Feature_24, Feature_26, Feature_27, Feature_28,\n         Feature_30\n         )\n\n# mutating as a factor for downsampling \npdf &lt;- pdf |&gt; \n  dplyr::mutate(Diagnosis = as.factor(Diagnosis))\n\n# class definition\ntarget &lt;- \"Diagnosis\"\n\n# downsampling\nset.seed(12345)\ndownsampled_df &lt;- downSample(x = pdf[, colnames(pdf) != target], y = pdf[[target]])\ndownsampled_df &lt;- cbind(downsampled_df, downsampled_df$Class)\ncolnames(downsampled_df)[ncol(downsampled_df)] &lt;- target\n\n# subset, mutate, and scale\ndf2 &lt;- pdf \ndf2 &lt;- df2 |&gt; \n  mutate(Diagnosis = as.character(Diagnosis),\n         Diagnosis = as.numeric(Diagnosis))\ndf2[, -1] &lt;- scale(df2[, -1])\n\n\nFinal summary statistics across the data set.\n\n\n\n\n\nData summary\n\n\nName\ndf2\n\n\nNumber of rows\n569\n\n\nNumber of columns\n22\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n22\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\nDiagnosis\n0\n1\n0.37\n0.48\n0.00\n0.00\n0.00\n1.00\n1.00\n▇▁▁▁▅\n\n\nFeature_1\n0\n1\n0.00\n1.00\n-2.03\n-0.69\n-0.21\n0.47\n3.97\n▂▇▃▁▁\n\n\nFeature_2\n0\n1\n0.00\n1.00\n-2.23\n-0.73\n-0.10\n0.58\n4.65\n▃▇▃▁▁\n\n\nFeature_3\n0\n1\n0.00\n1.00\n-1.98\n-0.69\n-0.24\n0.50\n3.97\n▃▇▃▁▁\n\n\nFeature_4\n0\n1\n0.00\n1.00\n-1.45\n-0.67\n-0.29\n0.36\n5.25\n▇▃▂▁▁\n\n\nFeature_6\n0\n1\n0.00\n1.00\n-1.61\n-0.75\n-0.22\n0.49\n4.56\n▇▇▂▁▁\n\n\nFeature_7\n0\n1\n0.00\n1.00\n-1.11\n-0.74\n-0.34\n0.53\n4.24\n▇▃▂▁▁\n\n\nFeature_8\n0\n1\n0.00\n1.00\n-1.26\n-0.74\n-0.40\n0.65\n3.92\n▇▃▂▁▁\n\n\nFeature_10\n0\n1\n0.00\n1.00\n-1.82\n-0.72\n-0.18\n0.47\n4.91\n▆▇▂▁▁\n\n\nFeature_11\n0\n1\n0.00\n1.00\n-1.06\n-0.62\n-0.29\n0.27\n8.90\n▇▁▁▁▁\n\n\nFeature_13\n0\n1\n0.00\n1.00\n-1.04\n-0.62\n-0.29\n0.24\n9.45\n▇▁▁▁▁\n\n\nFeature_14\n0\n1\n0.00\n1.00\n-0.74\n-0.49\n-0.35\n0.11\n11.03\n▇▁▁▁▁\n\n\nFeature_16\n0\n1\n0.00\n1.00\n-1.30\n-0.69\n-0.28\n0.39\n6.14\n▇▃▁▁▁\n\n\nFeature_18\n0\n1\n0.00\n1.00\n-1.91\n-0.67\n-0.14\n0.47\n6.64\n▇▇▁▁▁\n\n\nFeature_20\n0\n1\n0.00\n1.00\n-1.10\n-0.58\n-0.23\n0.29\n9.84\n▇▁▁▁▁\n\n\nFeature_21\n0\n1\n0.00\n1.00\n-1.73\n-0.67\n-0.27\n0.52\n4.09\n▆▇▃▁▁\n\n\nFeature_23\n0\n1\n0.00\n1.00\n-1.69\n-0.69\n-0.29\n0.54\n4.28\n▇▇▃▁▁\n\n\nFeature_24\n0\n1\n0.00\n1.00\n-1.22\n-0.64\n-0.34\n0.36\n5.92\n▇▂▁▁▁\n\n\nFeature_26\n0\n1\n0.00\n1.00\n-1.44\n-0.68\n-0.27\n0.54\n5.11\n▇▅▁▁▁\n\n\nFeature_27\n0\n1\n0.00\n1.00\n-1.30\n-0.76\n-0.22\n0.53\n4.70\n▇▅▂▁▁\n\n\nFeature_28\n0\n1\n0.00\n1.00\n-1.74\n-0.76\n-0.22\n0.71\n2.68\n▅▇▅▃▁\n\n\nFeature_30\n0\n1\n0.00\n1.00\n-1.60\n-0.69\n-0.22\n0.45\n6.84\n▇▃▁▁▁\n\n\n\n\n\n\nVisualization of variables as defined by the PCA across the first and second dimensions.\n\n\n\n# pca of subset data\nres.pca &lt;- PCA(df2, scale.unit = FALSE, graph = FALSE, ncp =4)\neig.val &lt;- get_eigenvalue(res.pca)\nvar &lt;- get_pca_var(res.pca)\n\n# Color by cos2 values: quality on the factor map\nfviz_pca_var(res.pca, col.var = \"contrib\",\n             gradient.cols = c(\"#00AFBB\", \"#E7B800\", \"#FC4E07\"), \n             repel = TRUE \n             )\n\n\n\n\n\n\n\nData Partitioning\n\nset.seed(12345)\n\n# splitting  \nind &lt;- sample(1:3, nrow(df2), replace=TRUE, prob=c(0.50, 0.25, 0.25))\ntrainData &lt;- df2[ind == 1, ]\ntestData &lt;- df2[ind == 2, ]\nxvalData &lt;- df2[ind == 3, ]\n\nModel Training and Visualization\nTraining of the neural network using the R library of neuralnet. This poweful algorithm “is based on the resilient backpropagation without weight backtracking and additionally modifies one learning rate, either the learningrate associated with the smallest absolute gradient (sag) or the smallest learningrate (slr) itself. The learning rates in the grprop algorithm are limited to the boundaries defined in learningrate.limit.”(Fritsch, Guenther, and Wright 2019)\n\n# neuralnet \nnn &lt;- neuralnet(Diagnosis ~., data = trainData, hidden = c(3),\n                lifesign = \"minimal\", linear.output = FALSE, likelihood=TRUE\n                # act.fct = \"tanh\", err.fct = \"sse\"\n                )\n\n# plot model\nplot(nn, radius = 0.03, arrow.length = 0.16, intercept = TRUE,\n     intercept.factor = 0.2, information = TRUE, information.pos = 8,\n     col.entry.synapse = \"black\", col.entry = \"maroon4\", line_stag= 0.1,\n     col.hidden = \"darkblue\", col.hidden.synapse = \"dimgrey\",\n     col.out = \"green\", col.out.synapse = \"blue\",\n     col.intercept = \"red\", fontsize = 9, dimension = 2,\n     show.weights = TRUE, rep = \"best\")\n\n\n\n\n\n\n\nModel’s Evaluation\nObservation & Notes | Section in-progress …\n\nApproach\n\nTraining\nTesting\nCross-validation\n\n\n\n\n# model evaluation\nmypredict &lt;- neuralnet::compute(nn, nn$covariate)$net.result\nmypredict &lt;- apply(mypredict, c(1), round)\n# confusion matrix - training set\nprint(table(mypredict[1:length(trainData$Diagnosis)], trainData$Diagnosis, dnn =c(\"Actual\",\"Predicted\")))\n\n      Predicted\nActual   0   1\n     0 158   0\n     1   0  97\n\n# accuracy(trainData$Diagnosis, mypredict[1:length(trainData$Diagnosis)]) #cross-entropy \n\n\n# model evaluation\ntestPred &lt;- neuralnet::compute(nn, testData[,1:22])$net.result\ntestPred &lt;- apply(testPred, c(1), round)\n# confusion matrix - test set\nprint(table(testPred[1:length(testData$Diagnosis)], testData$Diagnosis, dnn =c(\"Actual\", \"Predicted\")))\n\n      Predicted\nActual  0  1\n     0 97  5\n     1  2 56\n\n# accuracy(testPred[1:length(testData$Diagnosis)], testData$Diagnosis) #cross-entropy \n\n\n# model evaluation\nxvalPred &lt;- neuralnet::compute(nn, xvalData[,1:22])$net.result\nxvalPred &lt;- apply(xvalPred, c(1), round)\n# confusion matrix - xval set\nprint(table(xvalPred[1:length(xvalData$Diagnosis)], xvalData$Diagnosis, dnn =c(\"Actual\", \"Predicted\")))\n\n      Predicted\nActual  0  1\n     0 97  0\n     1  3 54\n\n# accuracy(xvalPred[1:length(xvalData$Diagnosis)], xvalData$Diagnosis) #cross-entropy \n\nMean Scores Comparison\nObservation & Notes | Section in-progress …\n\nn2 &lt;- (mypredict == trainData$Diagnosis)\nn3 &lt;- (testPred == testData$Diagnosis)\nn4 &lt;- (xvalPred == xvalData$Diagnosis)\n\nmean(n2); mean(n3); mean(n4)\n\n[1] 1\n\n\n[1] 0.95625\n\n\n[1] 0.9805195\n\n\nROC Curve\nObservation & Notes | Section in-progress …\n\n# ROC curve analysis \npred &lt;- neuralnet::compute(nn, nn$covariate)$net.result\npredObj &lt;- prediction(pred[1:length(xvalData$Diagnosis)], xvalData$Diagnosis)\nrocObj &lt;- performance(predObj, measure=\"tpr\", x.measure=\"fpr\")\naucObj &lt;- performance(predObj, measure = \"auc\")\nplot(rocObj,  main = \"ROC Curve\", cex.lab=1.25, cex.main = 1.5, col = \"blue\")\ntext(.75, .25, paste(\"Area under the curve:\", round(aucObj@y.values[[1]], 4)),\n     col = \"darkred\", cex = 1.25)\n\n\n\n\n\n\n\n\n\n\nReferences\n\nFritsch, Stefan, Frauke Guenther, and Marvin N. Wright. 2019. “Neuralnet: Training of Neural Networks.” https://CRAN.R-project.org/package=neuralnet.\n\n\nHenderson, I. Craig. 2015. “Breast Cancer,” October. https://doi.org/10.1093/med/9780199919987.001.0001.\n\n\nNarasimha, Aparna, B Vasavi, and ML Harendra Kumar. 2013. “Significance of Nuclear Morphometry in Benign and Malignant Breast Aspirates.” International Journal of Applied and Basic Medical Research 3 (1): 22. https://doi.org/10.4103/2229-516x.112237.\n\n\nWilliam Wolberg, Olvi Mangasarian. 1993. “Breast Cancer Wisconsin (Diagnostic).” UCI Machine Learning Repository. https://doi.org/10.24432/C5DW2B."
  },
  {
    "objectID": "blog_index.html",
    "href": "blog_index.html",
    "title": "Projects",
    "section": "",
    "text": "This is a repository of my previously completed assessments and data mining projects. The main idea is to represent career progression and exposure to the broad arena of data’s analytical theory and methods.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Projects\n\n\n\ndata science\n\n\nanalytics\n\n\nML\n\n\nAI\n\n\n\nThis is my portfolio as a data professional\n\n\n\nOscar Cardec\n\n\nJul 1, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHouse Market Analysis - Regression Model\n\n\n\nrandom forest\n\n\nregression\n\n\n\n\n\n\n\nOscar Cardec\n\n\nJul 21, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAicraft’s Lateral Deviation - Part 2\n\n\n\nsvm\n\n\nnaïve bayes\n\n\ngbm\n\n\nknn\n\n\nctree\n\n\nrpart\n\n\n\nAssessment of different classification algorithms and their respective results\n\n\n\nOscar Cardec\n\n\nNov 13, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAicraft’s Lateral Deviation - Part 1\n\n\n\nEDA\n\n\nETL\n\n\ndata\n\n\ndistribution\n\n\nsample\n\n\n\nAssessment of different classification algorithms and their respective results\n\n\n\nOscar Cardec\n\n\nNov 12, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nConditional Inference Trees\n\n\n\nctree\n\n\ndecision tree\n\n\nclassification\n\n\n\nSupervised learning classifications using the R package partykit conditional inference trees\n\n\n\nOscar Cardec\n\n\nOct 22, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTime Series Analysis - AAPL\n\n\n\ntime series\n\n\nforecasting\n\n\n\n\n\n\n\nOscar Cardec\n\n\nJul 15, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCancerous Cells Classification - Neural Network\n\n\n\nneural network\n\n\nclassification\n\n\npca\n\n\nctree\n\n\nrocr\n\n\nsplom\n\n\n\nMalignant cells classification across digitized images of fine needle aspirate (FNA) of a breast mass 3-dimensional fragmentated samples\n\n\n\nOscar Cardec\n\n\nJun 21, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGeospatial Analysis Infographic\n\n\n\ntableau\n\n\ninfographic\n\n\ngeospatial\n\n\n\nGDP analysis across Middle East (FY 2020)\n\n\n\nOscar Cardec\n\n\nApr 13, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJoin the Alliance\n\n\n\ninfographic\n\n\ntableau\n\n\n\n\n\n\n\nOscar Cardec\n\n\nMar 26, 2020\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "resume/index.html",
    "href": "resume/index.html",
    "title": "OSCAR CARDEC",
    "section": "",
    "text": "Williamsburg, Virginia\nocmain@gocardec.com\n443.296.2380\n\n\n\nHighly experienced Data Analyst and Machine Learning Specialist combining over 20 years of military experience and comprehensive knowledge across data analytics, machine learning, deep learning, and Generative AI. Specializing in data pipelines, data exploration, data pre-processing, data analysis, visualizations and data modeling with both structured and unstructured sources. Proven ability to translate stakeholder requirements and pain-points into actionable data science solutions. Expert in leading discussions across functions, building data science credibility, and mentoring teams in critical thinking, root cause analysis and problem-solving to enable decision-making at public and private sectors. Active security clearance and extensive experience supporting federal clients.\n\n\n\n• Machine Learning: Applied AI, predictive modeling, generative AI, and anomaly detection\n• Data Analysis: Exploratory analysis, data cleaning, statistical inferences, and complex visualizations\n• Strategic Leadership: Decision analysis, critical thinking, and project management\n• Technical Expertise: Advanced proficiency in Python, R, Power BI, and cloud-based tools\n\n\n\n\n\n• Performs hands-on analysis and modeling involving the creation of intervention hypotheses and experiments, assessment of data needs and available sources, determination of optimal analytical approaches, performance of exploratory data analysis, and feature generation\n• Collaborates with mission stakeholders to define, frame, and scope mission challenges. Strategize best way forward for extracting, cleaning, and transforming operational and transactional data to build descriptive and predictive models/analytics in order to inform mission directors. Capture data inferences and communicate them via Palantir Envision foundry, PowerBI interactive dashboards, and/or PowerApps customized applications.\n\n\n\n• Directed cloud solution research, market analysis, and data migration, ensured operational integrity.\n• Steered Enterprise Data Management Plan review and implementation in support of the ACC & DAF.\n• Developed strategies for data catalog and data governance in line with DoD VAULTIS guidelines.\n• Overhauled ML practices and data mining strategies; standardized decision-making, workflow analysis.\n• Created and optimized data taxonomies/ontologies, and audited cloud capabilities for analytic solutions.\n\n\n\n• Directed machine learning projects, utilizing data mining and anomaly detection to uncover insights.\n• Led quantitative analysis using operations research tools and time-series forecasting.\n• Conducted training on Splunk MLTK and integrated it into daily operations, transformed probabilistic measures to detect network traffic interruptions.\n\n\n\n• Spearheaded data analysis for DHS/CBP, optimizing assets allocations through historical data insights.\n• Implemented machine learning models and provided mentoring to junior staff.\n• Developed K-means algorithms for public health initiatives and led analytic proof of concept projects.\n• Advanced analytic practices for ACC Force Generation, improved data strategy and data sources ingestion.\n• Conducted social network analysis, topic modeling, and sentiment analysis, advanced analytical practices.\n\n\n\n• Sustained national security operations by employing novel natural language processing, social media and web content scraping, topic mining, and sentiment analysis techniques.\n• Utilized exploratory data analysis tradecraft and machine learning procedures to uncover hidden data, meeting client’s information requests.\n\n\n\n• Managed intelligence collection functions, improving operational capabilities by 85%.\n• Delivered over 1200 intelligence briefs to senior leadership.\n• Directed the allocation and training of 2,600 Intelligence Analysts, enhancing workforce efficiency.\n• Supervised 1,300 personnel in ISR missions and managed DCGS programs.\n• Led joint teams in foreign missile analysis and multi-source data extraction.\n• Oversaw counter-terrorism operations, assessing global targets and cyber objectives.\n• Directed counter-proliferation analysis and reporting, leading to international sanctions.\n• Coordinated Force Protection across national organizations, ensuring the safe return of personnel.\n\n\n\n\nMaster of Science in Data Analytics (MSDA) | University of Maryland, Global Campus\n• Integrated Support Vector Machine (SVM) and K-Nearest Neighbors (KNN) heuristic algorithms to enhance Federal Aviation Administration’s NextGen R&D Office aircraft’s lateral deviation anomaly detection tools.\n• Designed Conditional Inference Tree (CTREE) for classification and interpretation of CTGs, optimizing identification of at-risk newborns and their mothers.\n• Developed an irregular-cells Convolutional Neural Network (CNN) predictive model for early detection of breast carcinoma.\n• Classified Landsat Multispectral Scanner images using K-means model, base-lining image study with vector/centroid pixel- dimension features.\n\nBachelor of Science in Information Technology Management | American Military University\n• Applied industry driven techniques throughout complete IT project’s development lifecycle\n• Applied principles and practices of business solutions, database systems, networks, information systems, information security, and information technology project planning\n\n\n\n• Programming: Python, R, SQL, C#, MATLAB, SAS, VSC, Git, TensorFlow, Keras, PyTorch\n• Data Science Tools: Scikit-learn, Pandas, Numpy, Seaborn, Matplotlib, Plotly, Tidyverse\n• Cloud & BI: Azure, AWS, GCP, KNIME, Qlik, Tableau, Power BI, PowerApps, IBM Cognos Analytics, SAS E-Miner\n• Machine Learning: Time series forecasting, econometric models, classification techniques\n• Development: Shiny, Quarto, Rmarkdown, Dash, Boken, Streamlit, Flask\n\nData Science Additional Packages\nbeautifulsoup, caret, docker, dv-api, faker, geopandas, ggmap, gmodels, googlevis, ggvis, graphDB, H2O, iml, korpus, label studio, mobi, mysql, nltk, oneR, parallel, protégé, pyspark, pytorch, quanteda, ranger, scipy, scrapy, selenium, simpy, sdv, splunk mltk, spyder, spacy, sql, sqlite3, statsmodels, Tesseract OCR, tidygraph, tm, vip, xgboost, zoo"
  },
  {
    "objectID": "resume/index.html#professional-summary",
    "href": "resume/index.html#professional-summary",
    "title": "OSCAR CARDEC",
    "section": "",
    "text": "Highly experienced Data Analyst and Machine Learning Specialist combining over 20 years of military experience and comprehensive knowledge across data analytics, machine learning, deep learning, and Generative AI. Specializing in data pipelines, data exploration, data pre-processing, data analysis, visualizations and data modeling with both structured and unstructured sources. Proven ability to translate stakeholder requirements and pain-points into actionable data science solutions. Expert in leading discussions across functions, building data science credibility, and mentoring teams in critical thinking, root cause analysis and problem-solving to enable decision-making at public and private sectors. Active security clearance and extensive experience supporting federal clients."
  },
  {
    "objectID": "resume/index.html#core-competencies",
    "href": "resume/index.html#core-competencies",
    "title": "OSCAR CARDEC",
    "section": "",
    "text": "• Machine Learning: Applied AI, predictive modeling, generative AI, and anomaly detection\n• Data Analysis: Exploratory analysis, data cleaning, statistical inferences, and complex visualizations\n• Strategic Leadership: Decision analysis, critical thinking, and project management\n• Technical Expertise: Advanced proficiency in Python, R, Power BI, and cloud-based tools"
  },
  {
    "objectID": "resume/index.html#professional-experience",
    "href": "resume/index.html#professional-experience",
    "title": "OSCAR CARDEC",
    "section": "",
    "text": "• Performs hands-on analysis and modeling involving the creation of intervention hypotheses and experiments, assessment of data needs and available sources, determination of optimal analytical approaches, performance of exploratory data analysis, and feature generation\n• Collaborates with mission stakeholders to define, frame, and scope mission challenges. Strategize best way forward for extracting, cleaning, and transforming operational and transactional data to build descriptive and predictive models/analytics in order to inform mission directors. Capture data inferences and communicate them via Palantir Envision foundry, PowerBI interactive dashboards, and/or PowerApps customized applications.\n\n\n\n• Directed cloud solution research, market analysis, and data migration, ensured operational integrity.\n• Steered Enterprise Data Management Plan review and implementation in support of the ACC & DAF.\n• Developed strategies for data catalog and data governance in line with DoD VAULTIS guidelines.\n• Overhauled ML practices and data mining strategies; standardized decision-making, workflow analysis.\n• Created and optimized data taxonomies/ontologies, and audited cloud capabilities for analytic solutions.\n\n\n\n• Directed machine learning projects, utilizing data mining and anomaly detection to uncover insights.\n• Led quantitative analysis using operations research tools and time-series forecasting.\n• Conducted training on Splunk MLTK and integrated it into daily operations, transformed probabilistic measures to detect network traffic interruptions.\n\n\n\n• Spearheaded data analysis for DHS/CBP, optimizing assets allocations through historical data insights.\n• Implemented machine learning models and provided mentoring to junior staff.\n• Developed K-means algorithms for public health initiatives and led analytic proof of concept projects.\n• Advanced analytic practices for ACC Force Generation, improved data strategy and data sources ingestion.\n• Conducted social network analysis, topic modeling, and sentiment analysis, advanced analytical practices.\n\n\n\n• Sustained national security operations by employing novel natural language processing, social media and web content scraping, topic mining, and sentiment analysis techniques.\n• Utilized exploratory data analysis tradecraft and machine learning procedures to uncover hidden data, meeting client’s information requests.\n\n\n\n• Managed intelligence collection functions, improving operational capabilities by 85%.\n• Delivered over 1200 intelligence briefs to senior leadership.\n• Directed the allocation and training of 2,600 Intelligence Analysts, enhancing workforce efficiency.\n• Supervised 1,300 personnel in ISR missions and managed DCGS programs.\n• Led joint teams in foreign missile analysis and multi-source data extraction.\n• Oversaw counter-terrorism operations, assessing global targets and cyber objectives.\n• Directed counter-proliferation analysis and reporting, leading to international sanctions.\n• Coordinated Force Protection across national organizations, ensuring the safe return of personnel."
  },
  {
    "objectID": "resume/index.html#education",
    "href": "resume/index.html#education",
    "title": "OSCAR CARDEC",
    "section": "",
    "text": "Master of Science in Data Analytics (MSDA) | University of Maryland, Global Campus\n• Integrated Support Vector Machine (SVM) and K-Nearest Neighbors (KNN) heuristic algorithms to enhance Federal Aviation Administration’s NextGen R&D Office aircraft’s lateral deviation anomaly detection tools.\n• Designed Conditional Inference Tree (CTREE) for classification and interpretation of CTGs, optimizing identification of at-risk newborns and their mothers.\n• Developed an irregular-cells Convolutional Neural Network (CNN) predictive model for early detection of breast carcinoma.\n• Classified Landsat Multispectral Scanner images using K-means model, base-lining image study with vector/centroid pixel- dimension features.\n\nBachelor of Science in Information Technology Management | American Military University\n• Applied industry driven techniques throughout complete IT project’s development lifecycle\n• Applied principles and practices of business solutions, database systems, networks, information systems, information security, and information technology project planning"
  },
  {
    "objectID": "resume/index.html#technical-competencies",
    "href": "resume/index.html#technical-competencies",
    "title": "OSCAR CARDEC",
    "section": "",
    "text": "• Programming: Python, R, SQL, C#, MATLAB, SAS, VSC, Git, TensorFlow, Keras, PyTorch\n• Data Science Tools: Scikit-learn, Pandas, Numpy, Seaborn, Matplotlib, Plotly, Tidyverse\n• Cloud & BI: Azure, AWS, GCP, KNIME, Qlik, Tableau, Power BI, PowerApps, IBM Cognos Analytics, SAS E-Miner\n• Machine Learning: Time series forecasting, econometric models, classification techniques\n• Development: Shiny, Quarto, Rmarkdown, Dash, Boken, Streamlit, Flask\n\nData Science Additional Packages\nbeautifulsoup, caret, docker, dv-api, faker, geopandas, ggmap, gmodels, googlevis, ggvis, graphDB, H2O, iml, korpus, label studio, mobi, mysql, nltk, oneR, parallel, protégé, pyspark, pytorch, quanteda, ranger, scipy, scrapy, selenium, simpy, sdv, splunk mltk, spyder, spacy, sql, sqlite3, statsmodels, Tesseract OCR, tidygraph, tm, vip, xgboost, zoo"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Oscar Cardec",
    "section": "",
    "text": "Highly experienced Data Professional and Machine Learning Specialist with over 20 years of military experience and comprehensive knowledge across data analysis, visualizations, machine learning, deep learning, and Generative AI. Specializing in data pipelines, exploration, data cleaning and analysis, visualization, and modeling; with both structured and unstructured data sources. Expert in developing complex visualizations and leveraging advanced machine learning techniques to solve global challenges. Proven ability to translate complex data insights into actionable information for decision-making in public and private sectors.\n\n\n\n\nUniversity of Maryland, Global Campus - MS in Data Analytics (MSDA)\nAmerican Military University - BS in Information Technology Management\nCommunity College of the Air Force - AS in Intelligence Studies & Technology\n\n\n\n\nManTech | Data Scientist | Sep 2024 - Present\nWTI | R&D, Data Strategist | Jul 2023 - Sep 2024\nGovCIO | Data Scientist & ML Specialist | Feb 2023 - Jul 2023\nDeloitte & Touché, LLC | Lead Data Scientist | Apr 2021 - Feb 2023\nAccenture Federal Services | Data Science Intern | Oct 2020 - Mar 2021\nUS Air Force | Manager, Intelligence Analyst | 2013 - 2020\nUS Air Force | Information Manager & Database Administrator | 2007 - 2013\nUS Air Force | Aerospace Maintenance Craftsman | 2000 - 2007"
  },
  {
    "objectID": "index.html#welcome-to-my-portfolio",
    "href": "index.html#welcome-to-my-portfolio",
    "title": "Oscar Cardec",
    "section": "",
    "text": "Highly experienced Data Professional and Machine Learning Specialist with over 20 years of military experience and comprehensive knowledge across data analysis, visualizations, machine learning, deep learning, and Generative AI. Specializing in data pipelines, exploration, data cleaning and analysis, visualization, and modeling; with both structured and unstructured data sources. Expert in developing complex visualizations and leveraging advanced machine learning techniques to solve global challenges. Proven ability to translate complex data insights into actionable information for decision-making in public and private sectors.\n\n\n\n\nUniversity of Maryland, Global Campus - MS in Data Analytics (MSDA)\nAmerican Military University - BS in Information Technology Management\nCommunity College of the Air Force - AS in Intelligence Studies & Technology\n\n\n\n\nManTech | Data Scientist | Sep 2024 - Present\nWTI | R&D, Data Strategist | Jul 2023 - Sep 2024\nGovCIO | Data Scientist & ML Specialist | Feb 2023 - Jul 2023\nDeloitte & Touché, LLC | Lead Data Scientist | Apr 2021 - Feb 2023\nAccenture Federal Services | Data Science Intern | Oct 2020 - Mar 2021\nUS Air Force | Manager, Intelligence Analyst | 2013 - 2020\nUS Air Force | Information Manager & Database Administrator | 2007 - 2013\nUS Air Force | Aerospace Maintenance Craftsman | 2000 - 2007"
  },
  {
    "objectID": "resources/lindex.html",
    "href": "resources/lindex.html",
    "title": "Learning Plan: Data Science Mathematics",
    "section": "",
    "text": "This 12-week learning plan breaks down each topic into digestible weekly goals with recommended exercises and applications in data science."
  },
  {
    "objectID": "resources/lindex.html#structured-learning-plan-for-data-science-mathematics",
    "href": "resources/lindex.html#structured-learning-plan-for-data-science-mathematics",
    "title": "Learning Plan: Data Science Mathematics",
    "section": "",
    "text": "This 12-week learning plan breaks down each topic into digestible weekly goals with recommended exercises and applications in data science."
  },
  {
    "objectID": "resources/lindex.html#week-1-fundamentals-of-linear-algebra",
    "href": "resources/lindex.html#week-1-fundamentals-of-linear-algebra",
    "title": "Learning Plan: Data Science Mathematics",
    "section": "Week 1: Fundamentals of Linear Algebra",
    "text": "Week 1: Fundamentals of Linear Algebra\n📌 Topics\n- Introduction to Vectors and Matrices\n- Matrix Operations: Addition, Multiplication, Transpose\n- Identity, Inverse, and Special Matrices\n📚 Resources\n- Gilbert Strang’s Linear Algebra and Its Applications\n- Khan Academy: Linear Algebra Fundamentals\n📝 Exercises\n- Solve basic matrix operations and vector manipulations\n- Compute determinants and inverses manually and using NumPy\n🔍 Applications in Data Science\n- Representing datasets as matrices\n- Feature scaling and transformations"
  },
  {
    "objectID": "resources/lindex.html#week-2-advanced-linear-algebra-for-data-science",
    "href": "resources/lindex.html#week-2-advanced-linear-algebra-for-data-science",
    "title": "Learning Plan: Data Science Mathematics",
    "section": "Week 2: Advanced Linear Algebra for Data Science",
    "text": "Week 2: Advanced Linear Algebra for Data Science\n📌 Topics\n- Eigenvalues and Eigenvectors\n- Singular Value Decomposition (SVD)\n- Principal Component Analysis (PCA)\n📚 Resources\n- Deep Learning by Ian Goodfellow (Chapter on Linear Algebra)\n- 3Blue1Brown’s Essence of Linear Algebra series\n📝 Exercises\n- Compute eigenvalues and eigenvectors using Python\n- Perform PCA on a dataset using sklearn.decomposition.PCA\n🔍 Applications in Data Science\n- Dimensionality reduction in high-dimensional datasets\n- Image compression using SVD"
  },
  {
    "objectID": "resources/lindex.html#week-3-differential-calculus",
    "href": "resources/lindex.html#week-3-differential-calculus",
    "title": "Learning Plan: Data Science Mathematics",
    "section": "Week 3: Differential Calculus",
    "text": "Week 3: Differential Calculus\n📌 Topics\n- Limits and Continuity\n- Derivatives and Partial Derivatives\n- Chain Rule and Gradient Calculation\n📚 Resources\n- Calculus by James Stewart\n- MIT OpenCourseWare: Single Variable Calculus\n- 3Blue1Brown’s The essence of calculus series\n📝 Exercises\n- Manually compute derivatives of polynomial and exponential functions\n- Implement gradient computation in Python\n🔍 Applications in Data Science\n- Compute loss function gradients in machine learning"
  },
  {
    "objectID": "resources/lindex.html#week-4-integral-vector-calculus",
    "href": "resources/lindex.html#week-4-integral-vector-calculus",
    "title": "Learning Plan: Data Science Mathematics",
    "section": "Week 4: Integral & Vector Calculus",
    "text": "Week 4: Integral & Vector Calculus\n📌 Topics\n- Definite & Indefinite Integrals\n- Gradient, Hessian, and Jacobian Matrices\n- Applications in Probability\n📚 Resources\n- Mathematics for Machine Learning by Deisenroth et al.\n📝 Exercises\n- Compute multiple integrals manually and with Python (sympy.integrate())\n🔍 Applications in Data Science\n- Computing expectations in probability distributions"
  },
  {
    "objectID": "resources/lindex.html#week-5-probability-theory",
    "href": "resources/lindex.html#week-5-probability-theory",
    "title": "Learning Plan: Data Science Mathematics",
    "section": "Week 5: Probability Theory",
    "text": "Week 5: Probability Theory\n📌 Topics\n- Basics of Probability\n- Bayes’ Theorem & Conditional Probability\n- Discrete & Continuous Distributions\n📚 Resources\n- Probability and Statistics for Engineering and the Sciences by Jay Devore\n- Khan Academy: Probability & Statistics\n📝 Exercises\n- Solve probability problems manually\n- Simulate probability distributions using NumPy\n🔍 Applications in Data Science\n- Naïve Bayes classifier for text classification"
  },
  {
    "objectID": "resources/lindex.html#week-6-statistics-inference",
    "href": "resources/lindex.html#week-6-statistics-inference",
    "title": "Learning Plan: Data Science Mathematics",
    "section": "Week 6: Statistics & Inference",
    "text": "Week 6: Statistics & Inference\n📌 Topics\n- Descriptive Statistics\n- Hypothesis Testing (T-tests, Chi-Square)\n- Maximum Likelihood Estimation\n📚 Resources\n- The Elements of Statistical Learning by Hastie, Tibshirani, Friedman\n- Think Stats by Allen B. Downey\n📝 Exercises\n- Perform hypothesis testing using scipy.stats\n- Compute confidence intervals on sample datasets\n🔍 Applications in Data Science\n- Feature selection and A/B testing"
  },
  {
    "objectID": "resources/lindex.html#week-7-optimization-techniques",
    "href": "resources/lindex.html#week-7-optimization-techniques",
    "title": "Learning Plan: Data Science Mathematics",
    "section": "Week 7: Optimization Techniques",
    "text": "Week 7: Optimization Techniques\n📌 Topics\n- Gradient Descent & Variants (SGD, Adam, RMSprop)\n- Lagrange Multipliers\n- Convex vs. Non-Convex Optimization\n📚 Resources\n- Convex Optimization by Boyd & Vandenberghe\n📝 Exercises\n- Implement gradient descent from scratch in Python\n- Optimize logistic regression parameters using gradient descent\n🔍 Applications in Data Science\n- Training machine learning models efficiently"
  },
  {
    "objectID": "resources/lindex.html#week-8-numerical-methods",
    "href": "resources/lindex.html#week-8-numerical-methods",
    "title": "Learning Plan: Data Science Mathematics",
    "section": "Week 8: Numerical Methods",
    "text": "Week 8: Numerical Methods\n📌 Topics\n- Newton’s Method\n- Matrix Factorization (LU, QR)\n- Iterative Methods (Jacobi, Gauss-Seidel)\n📚 Resources\n- Numerical Methods for Scientists and Engineers by R.W. Hamming\n📝 Exercises\n- Solve non-linear equations using Newton’s method in Python\n🔍 Applications in Data Science\n- Efficient computations in large-scale datasets"
  },
  {
    "objectID": "resources/lindex.html#week-9-information-theory-entropy",
    "href": "resources/lindex.html#week-9-information-theory-entropy",
    "title": "Learning Plan: Data Science Mathematics",
    "section": "Week 9: Information Theory & Entropy",
    "text": "Week 9: Information Theory & Entropy\n📌 Topics\n- Entropy and Mutual Information\n- KL Divergence & Cross-Entropy\n- Shannon’s Theorem\n📚 Resources\n- Information Theory, Inference, and Learning Algorithms by David MacKay\n📝 Exercises\n- Compute entropy of a dataset using Python\n🔍 Applications in Data Science\n- Feature selection in decision trees"
  },
  {
    "objectID": "resources/lindex.html#week-10-graph-theory",
    "href": "resources/lindex.html#week-10-graph-theory",
    "title": "Learning Plan: Data Science Mathematics",
    "section": "Week 10: Graph Theory",
    "text": "Week 10: Graph Theory\n📌 Topics\n- Graph Representations & Adjacency Matrices\n- Graph Traversal (DFS, BFS)\n- PageRank Algorithm\n📚 Resources\n- Networks, Crowds, and Markets by Easley & Kleinberg\n📝 Exercises\n- Implement DFS and BFS in Python\n- Compute PageRank on a sample graph\n🔍 Applications in Data Science\n- Knowledge graph creation and analysis"
  },
  {
    "objectID": "resources/lindex.html#week-11-time-series-analysis",
    "href": "resources/lindex.html#week-11-time-series-analysis",
    "title": "Learning Plan: Data Science Mathematics",
    "section": "Week 11: Time Series Analysis",
    "text": "Week 11: Time Series Analysis\n📌 Topics\n- Stationarity & Differencing\n- Autoregressive Models (AR, MA, ARIMA)\n- Fourier Transforms\n📚 Resources\n- Time Series Analysis and Its Applications by Shumway & Stoffer\n📝 Exercises\n- Implement ARIMA models using statsmodels\n🔍 Applications in Data Science\n- Financial and economic forecasting"
  },
  {
    "objectID": "resources/lindex.html#week-12-final-project-consolidation",
    "href": "resources/lindex.html#week-12-final-project-consolidation",
    "title": "Learning Plan: Data Science Mathematics",
    "section": "Week 12: Final Project & Consolidation",
    "text": "Week 12: Final Project & Consolidation\n🎯 Objective: Apply all learned concepts in a capstone project\n📝 Project Ideas\n- Build a predictive model using PCA + Regression\n- Implement an ML model and optimize it using gradient descent\n- Perform statistical hypothesis testing on a real-world dataset"
  },
  {
    "objectID": "resources/books/index.html",
    "href": "resources/books/index.html",
    "title": "Recommended Books",
    "section": "",
    "text": "“Our job is obvious: We need to get out of the way, shine a light, and empower a new generation to teach itself and to go further and faster than any generation ever has.”\n~ Seth Godin\n\n\n\n\n\nApplied Predictive Modeling by Kuhn, Max, & Johnson, Kjell\n\n\nDesigning Data-Intensive Applications: The Big Ideas Behind Reliable, Scalable, and Maintainable Systems by Martin Kleppmann\n\n\n\nPractical Simulations for Machine Learning: Using Synthetic Data for AI by by Paris Buttfield-Addison, Mars Buttfield-Addison, Tim Nugent, & Jon Manning\n\n\nAdvanced R, Second Edition (Chapman & Hall/CRC The R Series) by Hadley Wickham\n\n\n\nDeep Learning (Adaptive Computation and Machine Learning Series) by Ian Goodfellow, Yoshua Bengio, & Aaron Courville\n\n\nData Engineering with Python: Work with massive datasets to design data models and automate data pipelines using Python by Paul Crickard\n\n\n\nPractical Statistics for Data Science: 50 Essential Concepts by Peter Bruce, & Andrew Bruce\n\n\nAn Introduction to Statistical Learning: with Applications in R by Gareth James, Daniela Witten, Trevor Hastie, & Robert Tibshirani\n\n\n\nThe Hundred-Page Machine Learning Book by Andriy Burkov\n\n\nMachine Learning Engineering by Andriy Burkov\n\n\n\nData Science from Scratch: First Principles with Python by Joel Grus\n\n\nHands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems\n\n\n\nData Science Projects with Python: A case study approach to gaining valuable insights from real data with machine learning, 2nd Edition by Stephen Klosterman\n\n\nThe Art of R Programming: A Tour of Statistical Software Design by Norman Matloft\n\n\n\nData Science for Business: What You Need to Know about Data Mining and Data-Analytic Thinking by Foster Provost & Tom Fawcett\n\n\nApplied Predictive Analytics: Principles and Techniques for the Professional Data Analyst by Dean Abbott\n\n\n\nA General Introduction to Data Analytics by João Moreira, Andre Carvalho, & Tomás Horvath\n\n\nData Science and Big Data Analytics: Discovering, Analyzing, Visualizing, and Presenting Data by EMC Education Services\n\n\n\nDecision Management Systems: A Practical Guide to Using Business Rules and Predictive Analytics by James Taylor\n\n\nSystems Analysis and Design Shelly Cashman by Scott Tilley & Harry Rosenblatt\n\n\n\nStorytelling with Data: A Data Visualization Guide for Business Professionals by Cole Nussbaumer Knaflic\n\n\nLearn Python 3 the Hard Way: A Very Simple Introduction to the Terrifying Beautiful World of Computer and Code by Zed Shaw\n\n\n\nPython for Data Analysis: Data Wrangling with Pandas, Numpy, and IPython by William McKinney\n\n\nR For Data Science: Import, Tidy, Transform, Visualize, and Model Data by Garrett Grolemund & Hadley Wickham\n\n\n\nMastering Shiny: Build Interactive Apps, Reports, and Dashboards Powered by R\n\n\nThe Proximity Principle: The Proven Strategy That Will Lead to a Career You Love\n\n\n\nSpark: The Definitive Guide: Big Data Processing Made Simple\n\n\nBuild a Career in Data Science\n\n\n\nHands-On Data Analysis with Pandas: A handbook for data collection, wrangling, analysis, and visualization\n\n\nHandbook of Parametric and Nonparametric Statistical Procedures, Fifth Edition"
  },
  {
    "objectID": "resources/mathforDS.html",
    "href": "resources/mathforDS.html",
    "title": "Mathematics & Techniques for Data Science",
    "section": "",
    "text": "This is an opinionated list of mathematical topics and techniques essential across the data science field. Although, not an all-inclusive list, knowledgement and mastering of these provide a solid foundation useful for understanding of the diverse branches sustaining machine learning and artificial intelligence.\n\n\nLinear Algebra\n\nVectors and Matrices\n\nVector Operations: Basic operations such as addition, subtraction, and scalar multiplication.\nMatrix Operations: Matrix multiplication, inversion, and transposition.\nTypes of Matrices: Special matrices like diagonal, symmetric, and orthogonal matrices.\n\nSystems of Linear Equations\n\nGaussian Elimination: Method for solving linear systems by reducing matrices to row echelon form.\nLU Decomposition: Factorization of a matrix into lower and upper triangular matrices.\n\nMatrix Decompositions\n\nEigenvalues and Eigenvectors: Key concepts for understanding matrix transformations.\nSingular Value Decomposition (SVD): Decomposition of a matrix into singular vectors and singular values.\nPrincipal Component Analysis (PCA): Technique for reducing the dimensionality of data.\n\nVector Spaces\n\nBasis and Dimension: Fundamental properties of vector spaces.\nSubspaces: Subsets of vector spaces that themselves are vector spaces.\nOrthogonality and Orthogonal Projections: Concepts for projecting vectors onto subspaces.\n\nLinear Transformations\n\nMatrix Representation: Representation of linear transformations using matrices.\nChange of Basis: Transforming coordinates from one basis to another.\n\n\n\n\nProbability and Statistics\n\nProbability Theory\n\nBasic Probability Concepts: Definitions and rules of probability.\nConditional Probability and Bayes’ Theorem: Probability of events given other events.\nRandom Variables: Variables whose values are subject to randomness.\nProbability Distributions: Descriptions of how probabilities are distributed over values.\nJoint, Marginal, and Conditional Distributions: Relationships between multiple random variables.\nExpectation, Variance, and Covariance: Measures of central tendency and variability.\n\nStatistical Inference\n\nPoint Estimation: Estimating population parameters from sample data.\nConfidence Intervals: Range of values within which a parameter is expected to lie.\nHypothesis Testing: Procedure for testing assumptions about population parameters.\np-values and Significance Levels: Metrics for assessing hypothesis test results.\nMaximum Likelihood Estimation (MLE): Method for estimating parameters by maximizing likelihood.\n\nBayesian Statistics\n\nBayesian Inference: Updating probabilities based on new data.\nPrior and Posterior Distributions: Distributions representing beliefs before and after observing data.\nMarkov Chain Monte Carlo (MCMC): Algorithms for sampling from complex distributions.\n\nRegression Analysis\n\nSimple and Multiple Linear Regression: Modeling relationships between variables.\nLogistic Regression: Modeling binary outcome variables.\nAssumptions and Diagnostics: Checking the validity of regression models.\n\nAdvanced Topics in Statistics\n\nTime Series Analysis: Analyzing data points collected over time.\nSurvival Analysis: Analyzing time-to-event data.\nNon-parametric Methods: Statistical methods not assuming a specific data distribution.\n\n\n\n\nNumerical Methods\n\nOptimization Techniques\n\nGradient Descent: Iterative method for finding local minima of functions.\nStochastic Gradient Descent: Variant of gradient descent using random subsets of data.\nConjugate Gradient Method: Optimization algorithm for large-scale linear systems.\nNewton’s Method: Iterative method for finding successively better approximations to roots.\n\nNumerical Linear Algebra\n\nMatrix Factorization: Decomposing matrices into products of simpler matrices.\nSolving Linear Systems: Methods for finding solutions to linear equations.\nEigenvalue Problems: Finding eigenvalues and eigenvectors of matrices.\n\n\n\n\nMachine Learning\n\nSupervised Learning\n\nRegression (Linear, Polynomial): Predicting continuous outcomes from input features.\nClassification (k-NN, SVM, Decision Trees, Random Forests): Categorizing data into predefined classes.\n\nUnsupervised Learning\n\nClustering (k-Means, Hierarchical, DBSCAN): Grouping similar data points together.\nDimensionality Reduction (PCA, t-SNE, LDA): Reducing the number of variables in data.\n\nModel Evaluation\n\nCross-Validation: Technique for assessing model performance on unseen data.\nROC Curves and AUC: Metrics for evaluating classification model performance.\nPrecision, Recall, F1-Score: Metrics for evaluating model accuracy and relevance.\n\nEnsemble Methods\n\nBagging and Boosting: Techniques for improving model performance by combining multiple models.\nRandom Forests: Ensemble learning method using multiple decision trees.\nGradient Boosting Machines (GBM, XGBoost): Powerful ensemble methods for regression and classification.\n\n\n\n\nNeural Networks and Deep Learning\n\nFundamentals of Neural Networks\n\nPerceptrons and Multilayer Perceptrons (MLP): Basic building blocks of neural networks.\nActivation Functions (ReLU, Sigmoid, Tanh): Functions introducing non-linearity into neural networks.\nBackpropagation and Gradient Descent: Algorithms for training neural networks.\n\nDeep Learning Architectures\n\nConvolutional Neural Networks (CNNs): Networks for processing grid-like data such as images.\nRecurrent Neural Networks (RNNs): Networks for processing sequential data.\nLong Short-Term Memory (LSTM): RNN variant for capturing long-term dependencies.\nGenerative Adversarial Networks (GANs): Networks for generating new, synthetic data.\n\nDeep Learning Techniques\n\nRegularization (Dropout, Batch Normalization): Techniques for preventing overfitting.\nTransfer Learning: Leveraging pre-trained models for new tasks.\nHyperparameter Tuning: Optimizing model parameters for better performance.\nAutoencoders: Networks for unsupervised learning of efficient codings.\n\nAdvanced Topics in Deep Learning\n\nAttention Mechanisms: Techniques for focusing on relevant parts of input data.\nTransformers: Architectures for handling sequential data with attention mechanisms.\nReinforcement Learning: Training models to make sequences of decisions.\n\n\n\n\nDimensionality Reduction\n\nPrincipal Component Analysis (PCA)\n\nEigenvalues and Eigenvectors: Key concepts for understanding PCA.\nVariance Explained: Measure of how much information is retained by principal components.\n\nSingular Value Decomposition (SVD)\n\nLow-Rank Approximations: Simplifying data by reducing its dimensionality.\n\nManifold Learning\n\nt-SNE (t-Distributed Stochastic Neighbor Embedding): Technique for visualizing high-dimensional data.\nUMAP (Uniform Manifold Approximation and Projection): Method for dimensionality reduction and visualization.\n\nFeature Selection and Extraction\n\nL1 Regularization (Lasso): Technique for feature selection in regression models.\nRecursive Feature Elimination: Method for selecting important features by recursively removing less important ones.\n\n\n\n\nAdditional Important Topics\n\nInformation Theory\n\nEntropy and Information Gain: Measures of uncertainty and information content.\nMutual Information: Measure of the mutual dependence between variables.\n\nGraph Theory\n\nGraph Representation: Ways to represent graphs using matrices and lists.\nGraph Algorithms (PageRank, Graph Neural Networks): Algorithms for processing graph-structured data.\n\nTime Series Analysis\n\nAutoregressive Models (AR, MA, ARIMA): Models for analyzing and forecasting time series data.\nSeasonal Decomposition: Breaking down time series data into seasonal components.\nForecasting Techniques: Methods for predicting future values in time series data.\n\nNatural Language Processing (NLP)\n\nText Preprocessing: Techniques for preparing text data for analysis.\nWord Embeddings (Word2Vec, GloVe): Methods for representing words as vectors.\nSequence Models (RNN, LSTM, Transformer): Models for processing and understanding sequential data."
  },
  {
    "objectID": "resources/mathforDS.html#math-techniques-for-ds---outline",
    "href": "resources/mathforDS.html#math-techniques-for-ds---outline",
    "title": "Mathematics/Techniques for Data Science",
    "section": "",
    "text": "This is an opinionated list of mathematical topics essential across the data science field. Although, not an all-inclusive list, it provides a solid foundations useful for understanding of the diverse branches of machine learning and artificial intelligence.\n\n\n\nVectors and Matrices\n\nVector Operations: Basic operations such as addition, subtraction, and scalar multiplication.\nMatrix Operations: Matrix multiplication, inversion, and transposition.\nTypes of Matrices: Special matrices like diagonal, symmetric, and orthogonal matrices.\n\nSystems of Linear Equations\n\nGaussian Elimination: Method for solving linear systems by reducing matrices to row echelon form.\nLU Decomposition: Factorization of a matrix into lower and upper triangular matrices.\n\nMatrix Decompositions\n\nEigenvalues and Eigenvectors: Key concepts for understanding matrix transformations.\nSingular Value Decomposition (SVD): Decomposition of a matrix into singular vectors and singular values.\nPrincipal Component Analysis (PCA): Technique for reducing the dimensionality of data.\n\nVector Spaces\n\nBasis and Dimension: Fundamental properties of vector spaces.\nSubspaces: Subsets of vector spaces that themselves are vector spaces.\nOrthogonality and Orthogonal Projections: Concepts for projecting vectors onto subspaces.\n\nLinear Transformations\n\nMatrix Representation: Representation of linear transformations using matrices.\nChange of Basis: Transforming coordinates from one basis to another.\n\n\n\n\n\n\nProbability Theory\n\nBasic Probability Concepts: Definitions and rules of probability.\nConditional Probability and Bayes’ Theorem: Probability of events given other events.\nRandom Variables: Variables whose values are subject to randomness.\nProbability Distributions: Descriptions of how probabilities are distributed over values.\nJoint, Marginal, and Conditional Distributions: Relationships between multiple random variables.\nExpectation, Variance, and Covariance: Measures of central tendency and variability.\n\nStatistical Inference\n\nPoint Estimation: Estimating population parameters from sample data.\nConfidence Intervals: Range of values within which a parameter is expected to lie.\nHypothesis Testing: Procedure for testing assumptions about population parameters.\np-values and Significance Levels: Metrics for assessing hypothesis test results.\nMaximum Likelihood Estimation (MLE): Method for estimating parameters by maximizing likelihood.\n\nBayesian Statistics\n\nBayesian Inference: Updating probabilities based on new data.\nPrior and Posterior Distributions: Distributions representing beliefs before and after observing data.\nMarkov Chain Monte Carlo (MCMC): Algorithms for sampling from complex distributions.\n\nRegression Analysis\n\nSimple and Multiple Linear Regression: Modeling relationships between variables.\nLogistic Regression: Modeling binary outcome variables.\nAssumptions and Diagnostics: Checking the validity of regression models.\n\nAdvanced Topics in Statistics\n\nTime Series Analysis: Analyzing data points collected over time.\nSurvival Analysis: Analyzing time-to-event data.\nNon-parametric Methods: Statistical methods not assuming a specific data distribution.\n\n\n\n\n\n\nOptimization Techniques\n\nGradient Descent: Iterative method for finding local minima of functions.\nStochastic Gradient Descent: Variant of gradient descent using random subsets of data.\nConjugate Gradient Method: Optimization algorithm for large-scale linear systems.\nNewton’s Method: Iterative method for finding successively better approximations to roots.\n\nNumerical Linear Algebra\n\nMatrix Factorization: Decomposing matrices into products of simpler matrices.\nSolving Linear Systems: Methods for finding solutions to linear equations.\nEigenvalue Problems: Finding eigenvalues and eigenvectors of matrices.\n\n\n\n\n\n\nSupervised Learning\n\nRegression (Linear, Polynomial): Predicting continuous outcomes from input features.\nClassification (k-NN, SVM, Decision Trees, Random Forests): Categorizing data into predefined classes.\n\nUnsupervised Learning\n\nClustering (k-Means, Hierarchical, DBSCAN): Grouping similar data points together.\nDimensionality Reduction (PCA, t-SNE, LDA): Reducing the number of variables in data.\n\nModel Evaluation\n\nCross-Validation: Technique for assessing model performance on unseen data.\nROC Curves and AUC: Metrics for evaluating classification model performance.\nPrecision, Recall, F1-Score: Metrics for evaluating model accuracy and relevance.\n\nEnsemble Methods\n\nBagging and Boosting: Techniques for improving model performance by combining multiple models.\nRandom Forests: Ensemble learning method using multiple decision trees.\nGradient Boosting Machines (GBM, XGBoost): Powerful ensemble methods for regression and classification.\n\n\n\n\n\n\nFundamentals of Neural Networks\n\nPerceptrons and Multilayer Perceptrons (MLP): Basic building blocks of neural networks.\nActivation Functions (ReLU, Sigmoid, Tanh): Functions introducing non-linearity into neural networks.\nBackpropagation and Gradient Descent: Algorithms for training neural networks.\n\nDeep Learning Architectures\n\nConvolutional Neural Networks (CNNs): Networks for processing grid-like data such as images.\nRecurrent Neural Networks (RNNs): Networks for processing sequential data.\nLong Short-Term Memory (LSTM): RNN variant for capturing long-term dependencies.\nGenerative Adversarial Networks (GANs): Networks for generating new, synthetic data.\n\nDeep Learning Techniques\n\nRegularization (Dropout, Batch Normalization): Techniques for preventing overfitting.\nTransfer Learning: Leveraging pre-trained models for new tasks.\nHyperparameter Tuning: Optimizing model parameters for better performance.\nAutoencoders: Networks for unsupervised learning of efficient codings.\n\nAdvanced Topics in Deep Learning\n\nAttention Mechanisms: Techniques for focusing on relevant parts of input data.\nTransformers: Architectures for handling sequential data with attention mechanisms.\nReinforcement Learning: Training models to make sequences of decisions.\n\n\n\n\n\n\nPrincipal Component Analysis (PCA)\n\nEigenvalues and Eigenvectors: Key concepts for understanding PCA.\nVariance Explained: Measure of how much information is retained by principal components.\n\nSingular Value Decomposition (SVD)\n\nLow-Rank Approximations: Simplifying data by reducing its dimensionality.\n\nManifold Learning\n\nt-SNE (t-Distributed Stochastic Neighbor Embedding): Technique for visualizing high-dimensional data.\nUMAP (Uniform Manifold Approximation and Projection): Method for dimensionality reduction and visualization.\n\nFeature Selection and Extraction\n\nL1 Regularization (Lasso): Technique for feature selection in regression models.\nRecursive Feature Elimination: Method for selecting important features by recursively removing less important ones.\n\n\n\n\n\n\nInformation Theory\n\nEntropy and Information Gain: Measures of uncertainty and information content.\nMutual Information: Measure of the mutual dependence between variables.\n\nGraph Theory\n\nGraph Representation: Ways to represent graphs using matrices and lists.\nGraph Algorithms (PageRank, Graph Neural Networks): Algorithms for processing graph-structured data.\n\nTime Series Analysis\n\nAutoregressive Models (AR, MA, ARIMA): Models for analyzing and forecasting time series data.\nSeasonal Decomposition: Breaking down time series data into seasonal components.\nForecasting Techniques: Methods for predicting future values in time series data.\n\nNatural Language Processing (NLP)\n\nText Preprocessing: Techniques for preparing text data for analysis.\nWord Embeddings (Word2Vec, GloVe): Methods for representing words as vectors.\nSequence Models (RNN, LSTM, Transformer): Models for processing and understanding sequential data."
  },
  {
    "objectID": "resources/mathforDS.html#section",
    "href": "resources/mathforDS.html#section",
    "title": "Mathematics/Techniques for Data Science",
    "section": "",
    "text": "This is an opinionated list of mathematical topics and techniques essential across the data science field. Although, not an all-inclusive list, knowledgement and mastering of these provide a solid foundation useful for understanding of the diverse branches sustaining machine learning and artificial intelligence.\n\n\n\nVectors and Matrices\n\nVector Operations: Basic operations such as addition, subtraction, and scalar multiplication.\nMatrix Operations: Matrix multiplication, inversion, and transposition.\nTypes of Matrices: Special matrices like diagonal, symmetric, and orthogonal matrices.\n\nSystems of Linear Equations\n\nGaussian Elimination: Method for solving linear systems by reducing matrices to row echelon form.\nLU Decomposition: Factorization of a matrix into lower and upper triangular matrices.\n\nMatrix Decompositions\n\nEigenvalues and Eigenvectors: Key concepts for understanding matrix transformations.\nSingular Value Decomposition (SVD): Decomposition of a matrix into singular vectors and singular values.\nPrincipal Component Analysis (PCA): Technique for reducing the dimensionality of data.\n\nVector Spaces\n\nBasis and Dimension: Fundamental properties of vector spaces.\nSubspaces: Subsets of vector spaces that themselves are vector spaces.\nOrthogonality and Orthogonal Projections: Concepts for projecting vectors onto subspaces.\n\nLinear Transformations\n\nMatrix Representation: Representation of linear transformations using matrices.\nChange of Basis: Transforming coordinates from one basis to another.\n\n\n\n\n\n\nProbability Theory\n\nBasic Probability Concepts: Definitions and rules of probability.\nConditional Probability and Bayes’ Theorem: Probability of events given other events.\nRandom Variables: Variables whose values are subject to randomness.\nProbability Distributions: Descriptions of how probabilities are distributed over values.\nJoint, Marginal, and Conditional Distributions: Relationships between multiple random variables.\nExpectation, Variance, and Covariance: Measures of central tendency and variability.\n\nStatistical Inference\n\nPoint Estimation: Estimating population parameters from sample data.\nConfidence Intervals: Range of values within which a parameter is expected to lie.\nHypothesis Testing: Procedure for testing assumptions about population parameters.\np-values and Significance Levels: Metrics for assessing hypothesis test results.\nMaximum Likelihood Estimation (MLE): Method for estimating parameters by maximizing likelihood.\n\nBayesian Statistics\n\nBayesian Inference: Updating probabilities based on new data.\nPrior and Posterior Distributions: Distributions representing beliefs before and after observing data.\nMarkov Chain Monte Carlo (MCMC): Algorithms for sampling from complex distributions.\n\nRegression Analysis\n\nSimple and Multiple Linear Regression: Modeling relationships between variables.\nLogistic Regression: Modeling binary outcome variables.\nAssumptions and Diagnostics: Checking the validity of regression models.\n\nAdvanced Topics in Statistics\n\nTime Series Analysis: Analyzing data points collected over time.\nSurvival Analysis: Analyzing time-to-event data.\nNon-parametric Methods: Statistical methods not assuming a specific data distribution.\n\n\n\n\n\n\nOptimization Techniques\n\nGradient Descent: Iterative method for finding local minima of functions.\nStochastic Gradient Descent: Variant of gradient descent using random subsets of data.\nConjugate Gradient Method: Optimization algorithm for large-scale linear systems.\nNewton’s Method: Iterative method for finding successively better approximations to roots.\n\nNumerical Linear Algebra\n\nMatrix Factorization: Decomposing matrices into products of simpler matrices.\nSolving Linear Systems: Methods for finding solutions to linear equations.\nEigenvalue Problems: Finding eigenvalues and eigenvectors of matrices.\n\n\n\n\n\n\nSupervised Learning\n\nRegression (Linear, Polynomial): Predicting continuous outcomes from input features.\nClassification (k-NN, SVM, Decision Trees, Random Forests): Categorizing data into predefined classes.\n\nUnsupervised Learning\n\nClustering (k-Means, Hierarchical, DBSCAN): Grouping similar data points together.\nDimensionality Reduction (PCA, t-SNE, LDA): Reducing the number of variables in data.\n\nModel Evaluation\n\nCross-Validation: Technique for assessing model performance on unseen data.\nROC Curves and AUC: Metrics for evaluating classification model performance.\nPrecision, Recall, F1-Score: Metrics for evaluating model accuracy and relevance.\n\nEnsemble Methods\n\nBagging and Boosting: Techniques for improving model performance by combining multiple models.\nRandom Forests: Ensemble learning method using multiple decision trees.\nGradient Boosting Machines (GBM, XGBoost): Powerful ensemble methods for regression and classification.\n\n\n\n\n\n\nFundamentals of Neural Networks\n\nPerceptrons and Multilayer Perceptrons (MLP): Basic building blocks of neural networks.\nActivation Functions (ReLU, Sigmoid, Tanh): Functions introducing non-linearity into neural networks.\nBackpropagation and Gradient Descent: Algorithms for training neural networks.\n\nDeep Learning Architectures\n\nConvolutional Neural Networks (CNNs): Networks for processing grid-like data such as images.\nRecurrent Neural Networks (RNNs): Networks for processing sequential data.\nLong Short-Term Memory (LSTM): RNN variant for capturing long-term dependencies.\nGenerative Adversarial Networks (GANs): Networks for generating new, synthetic data.\n\nDeep Learning Techniques\n\nRegularization (Dropout, Batch Normalization): Techniques for preventing overfitting.\nTransfer Learning: Leveraging pre-trained models for new tasks.\nHyperparameter Tuning: Optimizing model parameters for better performance.\nAutoencoders: Networks for unsupervised learning of efficient codings.\n\nAdvanced Topics in Deep Learning\n\nAttention Mechanisms: Techniques for focusing on relevant parts of input data.\nTransformers: Architectures for handling sequential data with attention mechanisms.\nReinforcement Learning: Training models to make sequences of decisions.\n\n\n\n\n\n\nPrincipal Component Analysis (PCA)\n\nEigenvalues and Eigenvectors: Key concepts for understanding PCA.\nVariance Explained: Measure of how much information is retained by principal components.\n\nSingular Value Decomposition (SVD)\n\nLow-Rank Approximations: Simplifying data by reducing its dimensionality.\n\nManifold Learning\n\nt-SNE (t-Distributed Stochastic Neighbor Embedding): Technique for visualizing high-dimensional data.\nUMAP (Uniform Manifold Approximation and Projection): Method for dimensionality reduction and visualization.\n\nFeature Selection and Extraction\n\nL1 Regularization (Lasso): Technique for feature selection in regression models.\nRecursive Feature Elimination: Method for selecting important features by recursively removing less important ones.\n\n\n\n\n\n\nInformation Theory\n\nEntropy and Information Gain: Measures of uncertainty and information content.\nMutual Information: Measure of the mutual dependence between variables.\n\nGraph Theory\n\nGraph Representation: Ways to represent graphs using matrices and lists.\nGraph Algorithms (PageRank, Graph Neural Networks): Algorithms for processing graph-structured data.\n\nTime Series Analysis\n\nAutoregressive Models (AR, MA, ARIMA): Models for analyzing and forecasting time series data.\nSeasonal Decomposition: Breaking down time series data into seasonal components.\nForecasting Techniques: Methods for predicting future values in time series data.\n\nNatural Language Processing (NLP)\n\nText Preprocessing: Techniques for preparing text data for analysis.\nWord Embeddings (Word2Vec, GloVe): Methods for representing words as vectors.\nSequence Models (RNN, LSTM, Transformer): Models for processing and understanding sequential data."
  }
]