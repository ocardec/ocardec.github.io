{
  "hash": "9d4b8597c271111feb67ae4a6414ef40",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"House Market Analysis - Regression Model\"\nauthor: \"Oscar Cardec\"\ndate: \"7/21/2022\"\ncategories: [linear regression, house market, r]\nimage: housing.jpeg\npage-layout: full\nfreeze: true\ncode-block-bg: true\ncode-block-border-left: \"#31BAE9\"\neditor_options: \n  chunk_output_type: inline\nbibliography: references.bib\n---\n\n\n\nHere is a quick descriptive analysis and potential regression model to build a comprehensive housing market analysis. The main purpose is to extract valuable insights related to any association between sales prices, home values, and/or additional property features and how these contribute to the overall value and sale of a property.\n\n### Data\n\nThe data employed here pertains to the houses found in a given California district and some summary stats about them based on the 1990 census data. This data was obtained from [Kaggle](https://www.kaggle.com/datasets/camnugent/california-housing-prices), and\n\nThe columns are as follows, their names are pretty self explanatory:\n\n-   longitude\n-   latitude\n-   housing_median_age\n-   total_rooms\n-   total_bedrooms\n-   population\n-   households\n-   median_income\n-   median_house_value\n-   ocean_proximity\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# if (!require(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(dplyr, readr, ggplot2, plotly, caret, hrbrthemes, plyr)\nmywd <- setwd(\"~/Documents/GitHub/ocquarto_portfolio/posts/house_market_analysis\")\nrawdf <- plyr::ldply(sel_files <- list.files(path = mywd, pattern = \"*.csv\", full.names = F), read_csv)\ndata <- rawdf\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# any missing values ?\nmissing_values <- sapply(data, function(x) sum(is.na(x)))\nmissing_values\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n         longitude           latitude housing_median_age        total_rooms \n                 0                  0                  0                  0 \n    total_bedrooms         population         households      median_income \n               207                  0                  0                  0 \nmedian_house_value    ocean_proximity \n                 0                  0 \n```\n\n\n:::\n:::\n\n\n\nThe variable **total_bedrooms**contains 207 NULL values. I could either remove them or imputation using the mean, median or other additional methods like KNN. Ultimately, I decide to omit those rows with NA values\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nclean_data <- na.omit(data)\n```\n:::\n\n\n\nAfter removing the null values, I opted to **scale the data** to maximize model results. The first step before scaling was to identify only the numeric variables.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# identify numeric columns and scale them\nnumeric_columns <- sapply(clean_data, is.numeric)\nscaled_data <- clean_data\nscaled_data[numeric_columns] <- scale(scaled_data[numeric_columns])\n```\n:::\n\n\n\nThe third data conditioning step was to identify and exclude any outliers.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# look for outliers, use Z-score\nz_scores <- as.data.frame(scale(scaled_data[numeric_columns]))\noutliers <- apply(z_scores, 2, function(x) sum(abs(x) > 3))\nhead(outliers)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n         longitude           latitude housing_median_age        total_rooms \n                 0                  0                  0                371 \n    total_bedrooms         population \n               368                339 \n```\n\n\n:::\n:::\n\n\n\nAs detailed on the above calculation, **total_rooms**, **total_bedrooms**, and **population** displayed outliers across the observations. Below, I created a simple function to calculated upper and lower boundaries, in this case +/- 3 stantandar deviations from the mean, and use these to cap any points outside of these fences.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# cap outliers at 3 stndvs\ncap_outliers <- function(x){\n  upper_limit <- mean(x) + 3*sd(x)\n  lower_limit <- mean(x) - 3*sd(x)\n  x <- ifelse(x > upper_limit, upper_limit, x)\n  x <- ifelse(x < lower_limit, lower_limit, x)\n  return(x)\n}\n\n# application of function \ncapped_data <- scaled_data\ncapped_data[numeric_columns] <- lapply(capped_data[numeric_columns], cap_outliers)\n```\n:::\n\n\n\n### Model Building and Diagnostics\n\nWith a cleaner data set, I was ready to start running my linear model and assess the quality of the product. As recorded below, I used **caret::createDataPartition** to split the train and test data subsets.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# libraries\nlibrary(caret)\nlibrary(car)\n\n# data splitting\nset.seed(1212)\ntrainIndex <- createDataPartition(capped_data$median_house_value, p = 0.7, list = FALSE)\ntrainData <- capped_data[trainIndex, ]\ntestData <- capped_data[-trainIndex, ]\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# fit multi-linear reg model \nmodel <- lm(median_house_value ~ ., data = trainData)\nsummary(model)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = median_house_value ~ ., data = trainData)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.4128 -0.3567 -0.0701  0.2566  3.6445 \n\nCoefficients:\n                           Estimate Std. Error t value Pr(>|t|)    \n(Intercept)                0.107657   0.008351  12.892  < 2e-16 ***\nlongitude                 -0.452254   0.020367 -22.205  < 2e-16 ***\nlatitude                  -0.454426   0.021440 -21.195  < 2e-16 ***\nhousing_median_age         0.132651   0.005588  23.737  < 2e-16 ***\ntotal_rooms               -0.339826   0.022130 -15.356  < 2e-16 ***\ntotal_bedrooms             0.674922   0.034828  19.379  < 2e-16 ***\npopulation                -0.500067   0.014669 -34.091  < 2e-16 ***\nhouseholds                 0.189897   0.034077   5.573 2.56e-08 ***\nmedian_income              0.764515   0.007521 101.657  < 2e-16 ***\nocean_proximityINLAND     -0.293399   0.017450 -16.814  < 2e-16 ***\nocean_proximityISLAND      0.964179   0.330276   2.919 0.003514 ** \nocean_proximityNEAR BAY   -0.066534   0.019089  -3.485 0.000493 ***\nocean_proximityNEAR OCEAN  0.024474   0.015683   1.561 0.118659    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5714 on 14291 degrees of freedom\nMultiple R-squared:  0.6736,\tAdjusted R-squared:  0.6733 \nF-statistic:  2458 on 12 and 14291 DF,  p-value: < 2.2e-16\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\npar(mfrow=c(2,2))\ndb <- \"darkblue\"\n# display residuals against fitted\nplot(model, which = 1, col = db)\n# normal Q-Q\nplot(model, which = 2, col = db)\n# scale-location (homogeneity of variance)\nplot(model, which = 3, col = db)\n# residuals against leverage (to identify influential points)\nplot(model, which = 5, col = db)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/diagnostics-1.png){width=960}\n:::\n:::\n\n\n\nUse of Variance Inflation Factor (VIF) to check for multicollinearity across the dataframe.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npar(mfrow = c(1,1))\n# check for multicollinearity using Variance Inflation Factor (VIF)\nvif(model)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                        GVIF Df GVIF^(1/(2*Df))\nlongitude          18.103182  1        4.254783\nlatitude           20.081131  1        4.481197\nhousing_median_age  1.372087  1        1.171361\ntotal_rooms        13.985712  1        3.739748\ntotal_bedrooms     37.594707  1        6.131452\npopulation          6.356390  1        2.521188\nhouseholds         36.337515  1        6.028061\nmedian_income       2.134262  1        1.460911\nocean_proximity     4.187460  4        1.196035\n```\n\n\n:::\n:::\n\n\n\nGiving the display of VIF values greater than 10, indicating potential multicollinearity, I decided to dig deeper to better understand the main contributors of variance, using the **FactoMineR** package for PCA.\n\n### Principal Component Analysis (PCA)\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(factoextra) #Easy PCA plots and visualization\nlibrary(FactoMineR) #PCA and factor analysis functions  \n# library(gridExtra) #Arranging multiple plot grids\n# library(gt) # For creating beautiful tables\n```\n:::\n\n\n\nAs the PCA method applies to numeric variables, I first identify only those variables and build an object to filter the rest of of the set.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# perform PCA\npcaData <- sapply(trainData, is.numeric)\n# exclude target variable\npcaData[\"median_house_value\"] <- FALSE\nres.pca <- PCA(trainData[pcaData], scale.unit = TRUE, ncp = 5, graph = FALSE)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# PCA results\nget_eig(res.pca)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      eigenvalue variance.percent cumulative.variance.percent\nDim.1 3.92150506       49.0188133                    49.01881\nDim.2 1.90816004       23.8520005                    72.87081\nDim.3 1.08513963       13.5642453                    86.43506\nDim.4 0.80078544       10.0098180                    96.44488\nDim.5 0.14829553        1.8536941                    98.29857\nDim.6 0.07972202        0.9965253                    99.29510\nDim.7 0.04183341        0.5229177                    99.81801\nDim.8 0.01455886        0.1819858                   100.00000\n```\n\n\n:::\n:::\n\n\n\nEigenvalues correspond to the amount of the variation explained by each principal component.[@factoextra]\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# visualize eigenvalues/variances\nfviz_screeplot(res.pca, addlabels = TRUE, ylim = c(0, 50))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# obtain the results for variables\nvar <- get_pca_var(res.pca)\nvar\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nPrincipal Component Analysis Results for variables\n ===================================================\n  Name       Description                                    \n1 \"$coord\"   \"Coordinates for the variables\"                \n2 \"$cor\"     \"Correlations between variables and dimensions\"\n3 \"$cos2\"    \"Cos2 for the variables\"                       \n4 \"$contrib\" \"contributions of the variables\"               \n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# control variable colors using their contributions\nfviz_pca_var(res.pca, col.var=\"contrib\",\n             gradient.cols = c(\"#00AFBB\", \"#E7B800\", \"#FC4E07\"),\n             repel = TRUE # avoid text overlapping\n             )\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# cos2 = the quality of the individuals on the factor map\n# fviz_pca_ind(res.pca, col.ind = \"cos2\",\n#              gradient.cols = c(\"#00AFBB\", \"#E7B800\", \"#FC4E07\"),\n#              repel = TRUE # avoid text overlapping (slow if many points)\n#              )\n```\n:::\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}